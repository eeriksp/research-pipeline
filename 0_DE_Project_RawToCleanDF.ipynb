{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278abcca",
   "metadata": {},
   "source": [
    "# Data Engineering Project \n",
    "## Importing the raw data, exporting the clean data\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb379",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fdb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Library Installations ##############\n",
    "# !pip install opendatasets # install the library for downloading the data set\n",
    "# ! pip install habanero\n",
    "\n",
    "################################################\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "### Specific-purpose libraries\n",
    "import opendatasets as od # downloading the data set from Kaggle\n",
    "from habanero import Crossref # CrossRef API\n",
    "\n",
    "### Misc\n",
    "import warnings # suppress warnings\n",
    "import time # tracking time\n",
    "import os # accessing directories\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d16ef7",
   "metadata": {},
   "source": [
    "## 1. Data import\n",
    "In order to download the data from Kaggle to a machine, it would be necessary to create a Kaggle API token. Make sure to include the `kaggle.json` fle in the same directory as this notebook.\n",
    "\n",
    "Some additional resources:\n",
    "- How to download the datasets from kaggle with `opendatasets` library https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/\n",
    "- Github repo for `opendatasets` library: https://github.com/JovianML/opendatasets\n",
    "\n",
    "First download the file (should be around `1.09 GB`. It will be stored in the `.arxiv/` directory. In case the file already exists, the download will be ignored with the `force = False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/Cornell-University/arxiv\", \n",
    "                     force = False # force = True downloads the file even if it finds a file with the same name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83699c1",
   "metadata": {},
   "source": [
    "Import the JSON file as pandas dataframe. For testing purposes, select how many rows are included. if `n_rows = \"all\"`, the entire data set is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2bac5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 257.28086519241333 seconds.\n",
      "Memory usage of raw df: 4.030794090591371 GB.\n",
      "Dataframe dimensions: (2146946, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       submitter  \\\n",
       "0  0704.0001  Pavel Nadolsky   \n",
       "1  0704.0002    Louis Theran   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                  comments               journal-ref  \\\n",
       "0  37 pages, 15 figures; published version  Phys.Rev.D76:013009,2007   \n",
       "1    To appear in Graphs and Combinatorics                      None   \n",
       "\n",
       "                          doi         report-no     categories  \\\n",
       "0  10.1103/PhysRevD.76.013009  ANL-HEP-PR-07-12         hep-ph   \n",
       "1                        None              None  math.CO cs.CG   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 'all'\n",
    "\n",
    "start_time = time.time()\n",
    "if n_rows == \"all\":\n",
    "    df = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True)\n",
    "else:\n",
    "    df = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True, nrows = n_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time elapsed: {end_time - start_time} seconds.')\n",
    "print(f'Memory usage of raw df: {df.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "print(f'Dataframe dimensions: {df.shape}')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e343f1f",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "In this step, data cleaning is performed. Here are the guidelines from the assignment:\n",
    "\n",
    "- You can drop the abstract as it is not required in the scope of this project,\n",
    "- You can drop publications with very short titles, e.g. one word, with empty authors\n",
    "\n",
    "What we do is we first drop all the columns that we are not planning to use in the project. Then, we are excluding the rows where works do not have a DOI. While we aknowledge that some valid publications do not have a DOI, a DOI demonstrates that this work is published (whether in a journal, as a pre-print, etc) and, hence, serves as a marker for publication quality. Finally, we exclude titles which have a length smaller than 10 characters - here, the main idea is to exclude all non-validly titled works, as <10 characters would amount to three words of three characters with two spaces - a rather rare title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf43e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2146946, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the abstract, submitter, comments, report-no, versions, journal-ref, and license, as these features are not used in this project\n",
    "## Of note, journal name will be retrieved later with a more standard label\n",
    "df = df.drop(['abstract', 'submitter', 'comments', 'report-no', 'license', 'versions', 'journal-ref'], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d70ce74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077424, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include only works with non-null values in doi\n",
    "df = df[~df['doi'].isnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e3de8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1077226, 7)\n",
      "Dataframe dimensions: (1077226, 7)\n",
      "Memory usage of raw pandas df: 0.6780343065038323 GB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0006</td>\n",
       "      <td>Y. H. Pong and C. K. Law</td>\n",
       "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
       "      <td>10.1103/PhysRevA.75.043613</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>[[Pong, Y. H., ], [Law, C. K., ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0007</td>\n",
       "      <td>Alejandro Corichi, Tatjana Vukasinac and Jose ...</td>\n",
       "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
       "      <td>10.1103/PhysRevD.76.044016</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            authors  \\\n",
       "0  0704.0001  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1  0704.0006                           Y. H. Pong and C. K. Law   \n",
       "2  0704.0007  Alejandro Corichi, Tatjana Vukasinac and Jose ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1  Bosonic characters of atomic Cooper pairs acro...   \n",
       "2  Polymer Quantum Mechanics and its Continuum Limit   \n",
       "\n",
       "                          doi         categories update_date  \\\n",
       "0  10.1103/PhysRevD.76.013009             hep-ph  2008-11-26   \n",
       "1  10.1103/PhysRevA.75.043613  cond-mat.mes-hall  2015-05-13   \n",
       "2  10.1103/PhysRevD.76.044016              gr-qc  2008-11-26   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1                  [[Pong, Y. H., ], [Law, C. K., ]]  \n",
       "2  [[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the publications with very short titles (less than 3 words)\n",
    "df = df[(df['title'].map(len) > 10)]\n",
    "df = df.reset_index(drop = True)\n",
    "print(df.shape)\n",
    "\n",
    "# Set the index of each paper to 'id'\n",
    "# df = df.set_index('id')\n",
    "print(f'Dataframe dimensions: {df.shape}')\n",
    "print(f'Memory usage of raw pandas df: {df.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf63da",
   "metadata": {},
   "source": [
    "### 2.1. Additional filtering by categories\n",
    "Here, we select only the articles that were published in the area of Computer Science (with prefix 'cs.'). First, we need to find all unique categories/classifier. Then, we extract a list of Computer Science disciplines. Finally, we filter the data so that only strictly Computer Science papers are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b72048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cs.AI', 'cs.DB', 'cs.HC', 'cs.DC', 'cs.OS', 'cs.MS', 'cs.NA',\n",
       "       'cs.DS', 'cs.FL', 'cs.GR', 'cs.NI', 'cs.NE', 'cs.MA', 'cs.CE',\n",
       "       'cs.AR', 'cs.SI', 'cs.CC', 'cs.RO', 'cs.SD', 'cs.OH', 'cs.PF',\n",
       "       'cs.IR', 'cs.GL', 'cs.SC', 'cs.LO', 'cs.LG', 'cs.SE', 'cs.PL',\n",
       "       'cs.DL', 'cs.CV', 'cs.CR', 'cs.SY', 'cs.DM', 'cs.GT', 'cs.CG',\n",
       "       'cs.CL', 'cs.CY', 'cs.IT', 'cs.MM', 'cs.ET'], dtype='<U5')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all unique categories and get the list of Computer Science categories\n",
    "cats = df.categories.str.split(',', expand = True)\n",
    "cats = set(pd.Series(np.sort(np.unique(cats))).str.split(' ', expand = True).values.flatten())\n",
    "cats = [str(n) for n in list(cats)]\n",
    "cs_cats = np.array([idx for idx in cats if idx.startswith('cs.')])\n",
    "cs_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2fb01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimensions: (27499, 7)\n",
      "Memory usage of raw pandas df: 0.01592199318110943 GB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0062</td>\n",
       "      <td>Rastislav \\v{S}r\\'amek, Bro\\v{n}a Brejov\\'a, T...</td>\n",
       "      <td>On-line Viterbi Algorithm and Its Relationship...</td>\n",
       "      <td>10.1007/978-3-540-74126-8_23</td>\n",
       "      <td>cs.DS</td>\n",
       "      <td>2010-01-25</td>\n",
       "      <td>[[Šrámek, Rastislav, ], [Brejová, Broňa, ], [V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0301</td>\n",
       "      <td>Akitoshi Kawamura</td>\n",
       "      <td>Differential Recursion and Differentially Alge...</td>\n",
       "      <td>10.1145/1507244.1507252</td>\n",
       "      <td>cs.CC</td>\n",
       "      <td>2009-04-19</td>\n",
       "      <td>[[Kawamura, Akitoshi, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.1267</td>\n",
       "      <td>Laurence Likforman-Sulem, Abderrazak Zahour, B...</td>\n",
       "      <td>Text Line Segmentation of Historical Documents...</td>\n",
       "      <td>10.1007/s10032-006-0023-z</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Likforman-Sulem, Laurence, ], [Zahour, Abder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.2344</td>\n",
       "      <td>Christian Vollaire (CEGELY), Laurent Nicolas (...</td>\n",
       "      <td>Parallel computing for the finite element method</td>\n",
       "      <td>10.1051/epjap:1998151</td>\n",
       "      <td>cs.DC</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Vollaire, Christian, , CEGELY], [Nicolas, La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.3238</td>\n",
       "      <td>Philippe Balbiani, Andreas Herzig and Nicolas ...</td>\n",
       "      <td>Alternative axiomatics and complexity of delib...</td>\n",
       "      <td>10.1007/s10992-007-9078-7</td>\n",
       "      <td>cs.LO</td>\n",
       "      <td>2011-04-29</td>\n",
       "      <td>[[Balbiani, Philippe, ], [Herzig, Andreas, ], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            authors  \\\n",
       "0  0704.0062  Rastislav \\v{S}r\\'amek, Bro\\v{n}a Brejov\\'a, T...   \n",
       "1  0704.0301                                  Akitoshi Kawamura   \n",
       "2  0704.1267  Laurence Likforman-Sulem, Abderrazak Zahour, B...   \n",
       "3  0704.2344  Christian Vollaire (CEGELY), Laurent Nicolas (...   \n",
       "4  0704.3238  Philippe Balbiani, Andreas Herzig and Nicolas ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  On-line Viterbi Algorithm and Its Relationship...   \n",
       "1  Differential Recursion and Differentially Alge...   \n",
       "2  Text Line Segmentation of Historical Documents...   \n",
       "3   Parallel computing for the finite element method   \n",
       "4  Alternative axiomatics and complexity of delib...   \n",
       "\n",
       "                            doi categories update_date  \\\n",
       "0  10.1007/978-3-540-74126-8_23      cs.DS  2010-01-25   \n",
       "1       10.1145/1507244.1507252      cs.CC  2009-04-19   \n",
       "2     10.1007/s10032-006-0023-z      cs.CV  2007-05-23   \n",
       "3         10.1051/epjap:1998151      cs.DC  2007-05-23   \n",
       "4     10.1007/s10992-007-9078-7      cs.LO  2011-04-29   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Šrámek, Rastislav, ], [Brejová, Broňa, ], [V...  \n",
       "1                           [[Kawamura, Akitoshi, ]]  \n",
       "2  [[Likforman-Sulem, Laurence, ], [Zahour, Abder...  \n",
       "3  [[Vollaire, Christian, , CEGELY], [Nicolas, La...  \n",
       "4  [[Balbiani, Philippe, ], [Herzig, Andreas, ], ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the articles that have a singular Computer Science domain tagged to article\n",
    "df = df[df['categories'].isin(cs_cats)].reset_index(drop = True)\n",
    "print(f'Dataframe dimensions: {df.shape}')\n",
    "print(f'Memory usage of raw pandas df: {df.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4c81b",
   "metadata": {},
   "source": [
    "# 3. Testing different write and read methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9a6f0",
   "metadata": {},
   "source": [
    "Now, the goal is to save a data frame that is relatively smaller but is also written and read fast. Hence, we try out different methods to achieve our goal. Below are the results which show that the optimal mathod of our choice is the `feather` format, as it provides relatively fast write and read, and the the size is only bigger than for `parquet` (which takes significantly more time for reads and writes). Some of the approaches are discussed here: # Saving options: https://stackoverflow.com/questions/48770542/what-is-the-difference-between-save-a-pandas-dataframe-to-pickle-and-to-csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aac4f9",
   "metadata": {},
   "source": [
    "### `feather`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1bbdd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to feather: 0.19532489776611328 seconds.\n",
      "feather file size is : 6.132837295532227 MB\n",
      "Time reading feather to pandas: 0.05029726028442383 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to feather\n",
    "feather_start = time.time()\n",
    "df.to_feather('df_clean.feather')\n",
    "feather_end = time.time()\n",
    "print(f'Time writing to feather: {feather_end - feather_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "feather_size = os.path.getsize('df_clean.feather')\n",
    "print(\"feather file size is :\", feather_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read feather\n",
    "feather_start = time.time()\n",
    "feather_read = pd.read_feather('df_clean.feather')\n",
    "feather_end = time.time()\n",
    "print(f'Time reading feather to pandas: {feather_end - feather_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857651a",
   "metadata": {},
   "source": [
    "### `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f82288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to parquet: 0.30479001998901367 seconds.\n",
      "parquet file size is : 4.419852256774902 MB\n",
      "Time reading parquet to pandas: 0.15041518211364746 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to parquet\n",
    "parquet_start = time.time()\n",
    "df.to_parquet('df_clean.parquet')\n",
    "parquet_end = time.time()\n",
    "print(f'Time writing to parquet: {parquet_end - parquet_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "parquet_size = os.path.getsize('df_clean.parquet')\n",
    "print(\"parquet file size is :\", parquet_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read parquet\n",
    "parquet_start = time.time()\n",
    "parquet_read = pd.read_parquet('df_clean.parquet')\n",
    "parquet_end = time.time()\n",
    "print(f'Time reading parquet to pandas: {parquet_end - parquet_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed4343",
   "metadata": {},
   "source": [
    "### `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127ee6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to csv: 0.21461033821105957 seconds.\n",
      "csv file size is : 8.174673080444336 MB\n",
      "Time reading csv to pandas: 0.09727215766906738 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to csv\n",
    "csv_start = time.time()\n",
    "df.to_csv('df_clean.csv')\n",
    "csv_end = time.time()\n",
    "print(f'Time writing to csv: {csv_end - csv_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "csv_size = os.path.getsize('df_clean.csv')\n",
    "print(\"csv file size is :\", csv_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read csv\n",
    "csv_start = time.time()\n",
    "csv_read = pd.read_csv('df_clean.csv')\n",
    "csv_end = time.time()\n",
    "print(f'Time reading csv to pandas: {csv_end - csv_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72a9ab",
   "metadata": {},
   "source": [
    "### `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2f8618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to pickle: 0.11856770515441895 seconds.\n",
      "pickle file size is : 8.388482093811035 MB\n",
      "Time reading pickle to pandas: 0.04011273384094238 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to pickle\n",
    "pickle_start = time.time()\n",
    "df.to_pickle('df_clean.pkl')\n",
    "pickle_end = time.time()\n",
    "print(f'Time writing to pickle: {pickle_end - pickle_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "pickle_size = os.path.getsize('df_clean.pkl')\n",
    "print(\"pickle file size is :\", pickle_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read feather\n",
    "pickle_start = time.time()\n",
    "pickle_read = pd.read_pickle('df_clean.pkl')\n",
    "pickle_end = time.time()\n",
    "print(f'Time reading pickle to pandas: {pickle_end - pickle_start} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
