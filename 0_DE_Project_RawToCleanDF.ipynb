{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278abcca",
   "metadata": {},
   "source": [
    "# Data Engineering Project \n",
    "## Importing the raw data, exporting the clean data\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb379",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fdb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Library Installations ##############\n",
    "# !pip install opendatasets # install the library for downloading the data set\n",
    "# ! pip install habanero\n",
    "\n",
    "################################################\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "\n",
    "### Specific-purpose libraries\n",
    "import opendatasets as od # downloading the data set from Kaggle\n",
    "from habanero import Crossref # CrossRef API\n",
    "\n",
    "### Misc\n",
    "import warnings # suppress warnings\n",
    "import time # tracking time\n",
    "import os # accessing directories\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d16ef7",
   "metadata": {},
   "source": [
    "## 1. Data import\n",
    "In order to download the data from Kaggle to a machine, it would be necessary to create a Kaggle API token. Make sure to include the `kaggle.json` fle in the same directory as this notebook.\n",
    "\n",
    "Some additional resources:\n",
    "- How to download the datasets from kaggle with `opendatasets` library https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/\n",
    "- Github repo for `opendatasets` library: https://github.com/JovianML/opendatasets\n",
    "\n",
    "First download the file (should be around `1.09 GB`. It will be stored in the `.arxiv/` directory. In case the file already exists, the download will be ignored with the `force = False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/Cornell-University/arxiv\", \n",
    "                     force = False # force = True downloads the file even if it finds a file with the same name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83699c1",
   "metadata": {},
   "source": [
    "Import the JSON file as pandas dataframe. For testing purposes, select how many rows are included. if `n_rows = \"all\"`, the entire data set is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bac5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 174.75681114196777 seconds.\n",
      "Memory usage of raw df: 4.030794090591371 GB.\n",
      "Dataframe dimensions: (2146946, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       submitter  \\\n",
       "0  0704.0001  Pavel Nadolsky   \n",
       "1  0704.0002    Louis Theran   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                  comments               journal-ref  \\\n",
       "0  37 pages, 15 figures; published version  Phys.Rev.D76:013009,2007   \n",
       "1    To appear in Graphs and Combinatorics                      None   \n",
       "\n",
       "                          doi         report-no     categories  \\\n",
       "0  10.1103/PhysRevD.76.013009  ANL-HEP-PR-07-12         hep-ph   \n",
       "1                        None              None  math.CO cs.CG   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 'all'\n",
    "\n",
    "start_time = time.time()\n",
    "if n_rows == \"all\":\n",
    "    df = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True)\n",
    "else:\n",
    "    df = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True, nrows = n_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time elapsed: {end_time - start_time} seconds.')\n",
    "print(f'Memory usage of raw df: {df.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "print(f'Dataframe dimensions: {df.shape}')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e343f1f",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "In this step, data cleaning is performed. Here are the guidelines from the assignment:\n",
    "\n",
    "- You can drop the abstract as it is not required in the scope of this project,\n",
    "- You can drop publications with very short titles, e.g. one word, with empty authors\n",
    "\n",
    "What we do is we first drop all the columns that we are not planning to use in the project. Then, we are excluding the rows where works do not have a DOI. While we aknowledge that some valid publications do not have a DOI, a DOI demonstrates that this work is published (whether in a journal, as a pre-print, etc) and, hence, serves as a marker for publication quality. Finally, we exclude titles which have a length smaller than 10 characters - here, the main idea is to exclude all non-validly titled works, as <10 characters would amount to three words of three characters with two spaces - a rather rare title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf43e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2146946, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the abstract, submitter, comments, report-no, versions, journal-ref, and license, as these features are not used in this project\n",
    "## Of note, journal name will be retrieved later with a more standard label\n",
    "df = df.drop(['abstract', 'submitter', 'comments', 'report-no', 'license', 'versions', 'journal-ref'], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d70ce74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077424, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include only works with non-null values in doi\n",
    "df = df[~df['doi'].isnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e3de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1077226, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0006</td>\n",
       "      <td>Y. H. Pong and C. K. Law</td>\n",
       "      <td>Bosonic characters of atomic Cooper pairs acro...</td>\n",
       "      <td>10.1103/PhysRevA.75.043613</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>[[Pong, Y. H., ], [Law, C. K., ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0007</td>\n",
       "      <td>Alejandro Corichi, Tatjana Vukasinac and Jose ...</td>\n",
       "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
       "      <td>10.1103/PhysRevD.76.044016</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            authors  \\\n",
       "0  0704.0001  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1  0704.0006                           Y. H. Pong and C. K. Law   \n",
       "2  0704.0007  Alejandro Corichi, Tatjana Vukasinac and Jose ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1  Bosonic characters of atomic Cooper pairs acro...   \n",
       "2  Polymer Quantum Mechanics and its Continuum Limit   \n",
       "\n",
       "                          doi         categories update_date  \\\n",
       "0  10.1103/PhysRevD.76.013009             hep-ph  2008-11-26   \n",
       "1  10.1103/PhysRevA.75.043613  cond-mat.mes-hall  2015-05-13   \n",
       "2  10.1103/PhysRevD.76.044016              gr-qc  2008-11-26   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1                  [[Pong, Y. H., ], [Law, C. K., ]]  \n",
       "2  [[Corichi, Alejandro, ], [Vukasinac, Tatjana, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the publications with very short titles (less than 3 words)\n",
    "df = df[(df['title'].map(len) > 10)]\n",
    "df = df.reset_index(drop = True)\n",
    "print(df.shape)\n",
    "\n",
    "# Set the index of each paper to 'id'\n",
    "# df = df.set_index('id')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0864ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimensions: (1077226, 7)\n",
      "Memory usage of raw pandas df: 0.6780343065038323 GB.\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataframe dimensions: {df.shape}')\n",
    "print(f'Memory usage of raw pandas df: {df.memory_usage(deep = True).sum()/1024/1024/1024} GB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4c81b",
   "metadata": {},
   "source": [
    "# 3. Testing different write and read methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9a6f0",
   "metadata": {},
   "source": [
    "Now, the goal is to save a data frame that is relatively smaller but is also written and read fast. Hence, we try out different methods to achieve our goal. Below are the results which show that the optimal mathod of our choice is the `feather` format, as it provides relatively fast write and read, and the the size is only bigger than for `parquet` (which takes significantly more time for reads and writes). Some of the approaches are discussed here: # Saving options: https://stackoverflow.com/questions/48770542/what-is-the-difference-between-save-a-pandas-dataframe-to-pickle-and-to-csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aac4f9",
   "metadata": {},
   "source": [
    "### `feather`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1bbdd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to feather: 2.7467079162597656 seconds.\n",
      "feather file size is : 293.9583911895752 MB\n",
      "Time reading feather to pandas: 2.473008871078491 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to feather\n",
    "feather_start = time.time()\n",
    "df.to_feather('df_clean.feather')\n",
    "feather_end = time.time()\n",
    "print(f'Time writing to feather: {feather_end - feather_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "feather_size = os.path.getsize('df_clean.feather')\n",
    "print(\"feather file size is :\", feather_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read feather\n",
    "feather_start = time.time()\n",
    "feather_read = pd.read_feather('df_clean.feather')\n",
    "feather_end = time.time()\n",
    "print(f'Time reading feather to pandas: {feather_end - feather_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857651a",
   "metadata": {},
   "source": [
    "### `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f82288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to parquet: 9.75090217590332 seconds.\n",
      "parquet file size is : 220.77945232391357 MB\n",
      "Time reading parquet to pandas: 12.483412027359009 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to parquet\n",
    "parquet_start = time.time()\n",
    "df.to_parquet('df_clean.parquet')\n",
    "parquet_end = time.time()\n",
    "print(f'Time writing to parquet: {parquet_end - parquet_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "parquet_size = os.path.getsize('df_clean.parquet')\n",
    "print(\"parquet file size is :\", parquet_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read parquet\n",
    "parquet_start = time.time()\n",
    "parquet_read = pd.read_parquet('df_clean.parquet')\n",
    "parquet_end = time.time()\n",
    "print(f'Time reading parquet to pandas: {parquet_end - parquet_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed4343",
   "metadata": {},
   "source": [
    "### `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127ee6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to csv: 10.570870637893677 seconds.\n",
      "csv file size is : 402.45714378356934 MB\n",
      "Time reading csv to pandas: 4.404193878173828 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to csv\n",
    "csv_start = time.time()\n",
    "df.to_csv('df_clean.csv')\n",
    "csv_end = time.time()\n",
    "print(f'Time writing to csv: {csv_end - csv_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "csv_size = os.path.getsize('df_clean.csv')\n",
    "print(\"csv file size is :\", csv_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read csv\n",
    "csv_start = time.time()\n",
    "csv_read = pd.read_csv('df_clean.csv')\n",
    "csv_end = time.time()\n",
    "print(f'Time reading csv to pandas: {csv_end - csv_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72a9ab",
   "metadata": {},
   "source": [
    "### `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2f8618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time writing to pickle: 31.464216709136963 seconds.\n",
      "pickle file size is : 411.2366714477539 MB\n",
      "Time reading pickle to pandas: 42.279184103012085 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Write to pickle\n",
    "pickle_start = time.time()\n",
    "df.to_pickle('df_clean.pkl')\n",
    "pickle_end = time.time()\n",
    "print(f'Time writing to pickle: {pickle_end - pickle_start} seconds.')\n",
    "\n",
    "# Size of written file\n",
    "pickle_size = os.path.getsize('df_clean.pkl')\n",
    "print(\"pickle file size is :\", pickle_size/1024/1024, \"MB\")\n",
    "\n",
    "# Read feather\n",
    "pickle_start = time.time()\n",
    "pickle_read = pd.read_pickle('df_clean.pkl')\n",
    "pickle_end = time.time()\n",
    "print(f'Time reading pickle to pandas: {pickle_end - pickle_start} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
