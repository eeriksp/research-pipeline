{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278abcca",
   "metadata": {},
   "source": [
    "# Data Engineering Project \n",
    "## Importing the raw data, exporting the clean data\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniv√§li\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb379",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fdb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "\n",
    "\n",
    "########### Library Installations ##############\n",
    "# !pip install opendatasets # install the library for downloading the data set\n",
    "# ! pip install habanero\n",
    "\n",
    "################################################\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "### Specific-purpose libraries\n",
    "import opendatasets as od # downloading the data set from Kaggle\n",
    "# from habanero import Crossref # CrossRef API\n",
    "\n",
    "### Misc\n",
    "import warnings # suppress warnings\n",
    "import time # tracking time\n",
    "import os # accessing directories\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d16ef7",
   "metadata": {},
   "source": [
    "## 1. Data Import\n",
    "In order to download the data from Kaggle to a machine, it would be necessary to create a Kaggle API token. Make sure to include the `kaggle.json` fle in the same directory as this notebook.\n",
    "\n",
    "Some additional resources:\n",
    "- How to download the datasets from kaggle with `opendatasets` library https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/\n",
    "- Github repo for `opendatasets` library: https://github.com/JovianML/opendatasets\n",
    "\n",
    "First download the file (should be around `1.09 GB`. It will be stored in the `.arxiv/` directory. In case the file already exists, the download will be ignored with the `force = False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df79a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of pipeline start: Fri Dec 16 11:38:22 2022\n"
     ]
    }
   ],
   "source": [
    "# Initialize the time of pipeline\n",
    "start_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(start_pipe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81a4058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./arxiv\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/Cornell-University/arxiv\", \n",
    "                     force = False # force = True downloads the file even if it finds a file with the same name\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83699c1",
   "metadata": {},
   "source": [
    "Import the JSON file as pandas dataframe. For testing purposes, select how many rows are included. if `n_rows = \"all\"`, the entire data set is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 'all'\n",
    "\n",
    "start_time = time.time()\n",
    "if n_rows == \"all\":\n",
    "    df_raw = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True)\n",
    "else:\n",
    "    df_raw  = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True, nrows = n_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time elapsed: {end_time - start_time} seconds.')\n",
    "print(f'Memory usage of raw df: {df_raw.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "print(f'Dataframe dimensions: {df_raw.shape}')\n",
    "df_raw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e343f1f",
   "metadata": {},
   "source": [
    "## 2. Preliminary Data Cleaning\n",
    "In this step, data cleaning is performed. Here are the guidelines from the assignment:\n",
    "\n",
    "- You can drop the abstract as it is not required in the scope of this project,\n",
    "- You can drop publications with very short titles, e.g. one word, with empty authors\n",
    "\n",
    "What we do is we first drop all the columns that we are not planning to use in the project. Then, we are excluding the rows where works do not have a DOI. While we aknowledge that some valid publications do not have a DOI, a DOI demonstrates that this work is published (whether in a journal, as a pre-print, etc) and, hence, serves as a marker for publication quality. Finally, we exclude titles which have a length smaller than 10 characters - here, the main idea is to exclude all non-validly titled works, as <10 characters would amount to three words of three characters with two spaces - a rather rare title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf43e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the abstract, submitter, comments, report-no, versions, journal-ref, and license, as these features are not used in this project\n",
    "## Of note, journal name will be retrieved later with a more standard label\n",
    "df_raw = df_raw.drop(['abstract', 'submitter', 'comments', 'report-no', 'license', 'versions', 'journal-ref'], axis = 1)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7291ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates \n",
    "df_raw = df_raw.drop_duplicates(subset=['id'])\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only works with non-null values in doi\n",
    "df_raw = df_raw[~df_raw['doi'].isnull()]\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3de8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the publications with very short titles (less than 3 words)\n",
    "df_raw = df_raw[(df_raw['title'].map(len) > 10)]\n",
    "df_raw = df_raw.reset_index(drop = True)\n",
    "print(df_raw.shape)\n",
    "\n",
    "# Set the index of each paper to 'id'\n",
    "# df = df.set_index('id')\n",
    "print(f'Dataframe dimensions: {df_raw.shape}')\n",
    "print(f'Memory usage of raw pandas df: {df_raw.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf63da",
   "metadata": {},
   "source": [
    "## 3. Fact and Dimension tables for Data Warehouse (DWH)\n",
    "\n",
    "Here, we create the tables with placeholder columns. In this data schema, we are using two factless fact tables: `authorship` that links articles (and its properties) with authors, and `article_category` which reflects scientific domain information for each article.\n",
    "\n",
    "**Fact table** <br>\n",
    "- `authorship`: links articles to authors\n",
    "    - `article_id`: VARCHAR article id (allows to retrieve this id from the original, raw df)\n",
    "    - `author_id`: VARCHAR composed from author's last name and first name initial (e.g., LastF)\n",
    "    \n",
    "    \n",
    "- `article_category`: links articles to authors\n",
    "    - `article_id`: VARCHAR article id (allows to retrieve this id from the original, raw df)\n",
    "    - `category_id`: VARCHAR composed from author's last name and first name initial (e.g., LastF)\n",
    "\n",
    "**Dimension tables** <br>\n",
    "- `article`: contains the information about all unique publications and links the dimension tables. The columns are:\n",
    "    - PK `article_id`: VARCHAR article id (allows to retrieve this id from the original, raw df)\n",
    "    - `title`: VARCHAR article title\n",
    "    - `doi`: VARCHAR article DOI\n",
    "    - `journal_id`:VARCHAR journal ID based on ISSN linking to the `journal` table\n",
    "    - `year`: INT year of publication\n",
    "    - `n_cites`: INT the number of citations (FACT)\n",
    "    - `n_authors`: INT the number of co-authors\n",
    "    \n",
    "\n",
    "- `author`: includes all individual authors of publications.\n",
    "    - PK `author_id`: VARCHAR composed from author's last name and first name initial (e.g., LastF)\n",
    "    - `lastname`: VARCHAR author's last name \n",
    "    - `first`: VARCHAR author's first name initial\n",
    "    - `middle`: VARCHAR author's middle name initial (if any)\n",
    "    - `gender`: INT (1 or 0), denoting 'Female' and 'Male', respectively (AUGMENTED VIA API!)\n",
    "    - `affiliation`: VARCHAR author's affiliation (AUGMENTED VIA API!)\n",
    "    - `hindex`: VARCHAR author's hindex (AUGMENTED VIA API OR COMPUTED (N PAPERS W/ N CITES)!\n",
    "    \n",
    "    \n",
    "- `journal`: includes all unique journals in which works were published\n",
    "    - PK `journal_id`: VARCHAR journal ID\n",
    "    - `issn`: VARCHAR journal ISSN (necessary for augmentation)\n",
    "    - `title`: VARCHAR journal title\n",
    "    - `if_latest`: FLOAT journal's latest Impact Factor (AUGMENTED VIA API!)\n",
    "\n",
    "- `category`: includes categories associated with articles\n",
    "    - PK `category_id`: VARCHAR\n",
    "    - `superdom`: VARCHAR super-domain of the category\n",
    "    - `subdom`: VARCHAR sub-domain of the category\n",
    "    \n",
    "    \n",
    "The DWH ERD figure is below:\n",
    "\n",
    "<img src=\"images/dwh_erd.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5606b5",
   "metadata": {},
   "source": [
    "**<font color = 'red'> USE A TEST DATA SET OF 1000 SAMPLES: </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data for small-scale testing\n",
    "df = df_raw.iloc[:1000,:] # Take a thousand rows for testing\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f57690",
   "metadata": {},
   "source": [
    "### 3.1. Factless fact tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d150af1",
   "metadata": {},
   "source": [
    "#### 3.1.1. Factless fact table: `authorship`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table fro article id and authors list\n",
    "## NB! Creating `authorship_raw` - for later authors extraction\n",
    "authorship_raw = df[['id', 'authors_parsed']].set_index('id')\n",
    "authorship_raw['n_authors'] = authorship_raw['authors_parsed'].str.len()\n",
    "authorship_raw = pd.DataFrame(authorship_raw['authors_parsed'].explode()).reset_index()\n",
    "\n",
    "# Create additional columns\n",
    "authorship_raw['last_name'] = np.nan\n",
    "authorship_raw['first_name'] = np.nan\n",
    "authorship_raw['middle_name'] = np.nan\n",
    "\n",
    "# Update the last, first, and middle names\n",
    "for i in range(len(authorship_raw)):\n",
    "    authorship_raw['last_name'][i] = authorship_raw['authors_parsed'][i][0]\n",
    "    authorship_raw['first_name'][i] = authorship_raw['authors_parsed'][i][1]\n",
    "    authorship_raw['middle_name'][i] = authorship_raw['authors_parsed'][i][2]\n",
    "\n",
    "# Drop the redundant column\n",
    "authorship_raw = authorship_raw.drop(columns = 'authors_parsed')\n",
    "\n",
    "# Author_identifier\n",
    "authorship_raw['author_id'] = authorship_raw['last_name'] + authorship_raw['first_name'].str[0]\n",
    "# Rename article id column\n",
    "authorship_raw = authorship_raw.rename({'id':'article_id'}, axis = 1)\n",
    "\n",
    "# Final table\n",
    "authorship = authorship_raw.drop(columns = ['last_name', 'first_name', 'middle_name'])\n",
    "\n",
    "print(f'Dataframe dimensions: {authorship.shape}')\n",
    "print(f'Memory usage of raw pandas df: {authorship.memory_usage(deep = True).sum()/1024/1024/1024} GB.')\n",
    "authorship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27585305",
   "metadata": {},
   "source": [
    "#### 3.1.2. Factless fact table: `article_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article-category factless fact table\n",
    "article_category = df[['id', 'categories']].set_index('id')\n",
    "article_category = pd.DataFrame(article_category['categories'].str.split(' ').explode()) # extract category codes for articles in long-df\n",
    "article_category = article_category.reset_index()\n",
    "\n",
    "article_category = article_category.rename(columns = {'id':'article_id', 'categories':'category_id'})\n",
    "\n",
    "print(f'Dataframe dimensions: {article_category.shape}')\n",
    "print(f'Memory usage of raw pandas df: {article_category.memory_usage(deep = True).sum()/1024/1024} MB.')\n",
    "article_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5b4f8",
   "metadata": {},
   "source": [
    "### 3.2. Dimensions tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10532ad5",
   "metadata": {},
   "source": [
    "#### 3.2.1. Dimension table: `article`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = pd.DataFrame(columns = ['article_id', 'title', 'doi', 'n_authors', 'journal_issn', 'n_cites', 'year'])\n",
    "article['article_id'] = df['id']\n",
    "article['title'] = df['title']\n",
    "article['doi'] = df['doi']\n",
    "article['n_authors'] = df['authors_parsed'].str.len() # get the number of authors\n",
    "article['year'] = df['update_date'].str.split('-').map(lambda x: x[0]).astype(int)\n",
    "#article = article.drop(column = 'date')\n",
    "\n",
    "print(f'Dataframe dimensions: {article.shape}')\n",
    "print(f'Memory usage of raw pandas df: {article.memory_usage(deep = True).sum()/1024/1024} MB.')\n",
    "article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81abd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbcea0c",
   "metadata": {},
   "source": [
    "#### 3.2.2. Dimension table: `author`\n",
    "NB! Dependency on `authorship_raw` table, i.e., data is extracted from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table from the `authorship` table\n",
    "author = authorship_raw[['author_id', 'last_name', 'first_name', 'middle_name']]\n",
    "\n",
    "# Drop duplicates\n",
    "author.drop_duplicates(keep=False,inplace=True)\n",
    "\n",
    "# Add the `gender` column to be augmented\n",
    "author['gender'] = np.nan\n",
    "author['affiliation'] = np.nan\n",
    "author['hindex'] = np.nan\n",
    "\n",
    "# Sort alphabetically by last name\n",
    "author = author.sort_values('author_id').reset_index(drop = True)\n",
    "\n",
    "# Final table\n",
    "print(f'Dataframe dimensions: {author.shape}')\n",
    "print(f'Memory usage of raw pandas df: {author.memory_usage(deep = True).sum()/1024/1024} MB.')\n",
    "author.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5132d3",
   "metadata": {},
   "source": [
    "#### 3.2.3. Dimension table: `journal `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f354cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal = pd.DataFrame(columns = ['journal_id', 'issn', 'title', 'if_latest'])\n",
    "\n",
    "print(f'Dataframe dimensions: {journal.shape}')\n",
    "print(f'Memory usage of raw pandas df: {journal.memory_usage(deep = True).sum()/1024/1024} MB.')\n",
    "journal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92a604",
   "metadata": {},
   "source": [
    "#### 3.2.4. Dimension table: `category`\n",
    "NB! Dependency on `article_category` table, i.e., data is extracted from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories dimension table\n",
    "category = pd.DataFrame(article_category['category_id'].copy().reset_index(drop = True))\n",
    "category[['superdom', 'subdom']] = category['category_id'].str.split('.', expand = True) # exract supr- and subdomain\n",
    "category = category.drop_duplicates() # drop duplicate rows\n",
    "category = category.sort_values('category_id').reset_index(drop = True) # sort values, reset index\n",
    "\n",
    "print(f'Dataframe dimensions: {category.shape}')\n",
    "print(f'Memory usage of raw pandas df: {category.memory_usage(deep = True).sum()/1024/1024} MB.')\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54ba22",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## article_category\n",
    "## category\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender and affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634ca15-df42-4281-85d2-d6425d5e0bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dba2b-b946-4df0-9400-d13931f11861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory 'tables'\n",
    "!mkdir tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c7b42-4490-4580-baac-0d508201cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorship.to_csv('tables/authorship.csv', index = False)\n",
    "article_category.to_csv('tables/article_category.csv', index = False)\n",
    "category.to_csv('tables/category.csv', index = False)\n",
    "journal.to_csv('tables/journal.csv', index = False)\n",
    "article.to_csv('tables/article.csv', index = False)\n",
    "author.to_csv('tables/author.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb1a21",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f16e9b-59ad-4228-9f63-262caf4c1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "authorship = pd.read_csv('tables/authorship.csv')\n",
    "article_category = pd.read_csv('tables/article_category.csv')\n",
    "category = pd.read_csv('tables/category.csv')\n",
    "journal = pd.read_csv('tables/journal.csv')\n",
    "article = pd.read_csv('tables/article.csv')\n",
    "author = pd.read_csv('tables/author.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdf7f4-6b0d-422f-b47f-ef8cf960d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f21ed2-6242-4fc1-a470-738e700a767e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e947cbe-2492-4ab3-8913-d93fe37dd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af407c-d558-4db7-8462-0f517a4b0930",
   "metadata": {},
   "source": [
    "# Drop Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42449003-4ae7-40dd-ac46-a80009ef0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "authorship_drop = \"DROP TABLE IF EXISTS authorship;\"\n",
    "article_category_drop = \"DROP TABLE IF EXISTS article_category;\"\n",
    "category_drop = \"DROP TABLE IF EXISTS category;\"\n",
    "journal_drop = \"DROP TABLE IF EXISTS journal;\"\n",
    "article_drop = \"DROP TABLE IF EXISTS article;\"\n",
    "author_drop = \"DROP TABLE IF EXISTS author;\"\n",
    "\n",
    "drop_tables = [authorship_drop, article_category_drop, category_drop, journal_drop, article_drop, author_drop]\n",
    "\n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d295154-cef9-4072-a8e0-c0dbe1c49397",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb500397-7d81-45c3-9107-eeee41f10836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tables\n",
    "## authorship\n",
    "authorship_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS authorship\n",
    "(article_id VARCHAR, \n",
    "author_id VARCHAR,\n",
    "PRIMARY KEY (article_id, author_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "## article\n",
    "article_category_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS article_category\n",
    "(article_id VARCHAR, \n",
    "category_id VARCHAR,\n",
    "PRIMARY KEY (article_id, category_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "## category\n",
    "category_create =  (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS category\n",
    "(category_id VARCHAR,\n",
    "superdom VARCHAR,\n",
    "subdom VARCHAR,\n",
    "PRIMARY KEY (category_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "## article\n",
    "article_create =  (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS article\n",
    "(article_id VARCHAR,\n",
    "title VARCHAR,\n",
    "doi VARCHAR,\n",
    "n_authors INT,\n",
    "journal_issn VARCHAR,\n",
    "n_cites VARCHAR,\n",
    "year VARCHAR,\n",
    "PRIMARY KEY (article_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "## author\n",
    "author_create =  (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS author\n",
    "(author_id VARCHAR,\n",
    "last_name VARCHAR,\n",
    "first_name VARCHAR,\n",
    "middle_name VARCHAR,\n",
    "gender VARCHAR,\n",
    "affiliation VARCHAR,\n",
    "hindex VARCHAR,\n",
    "PRIMARY KEY (author_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "## journal\n",
    "journal_create =  (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS journal\n",
    "(journal_id VARCHAR,\n",
    "issn VARCHAR,\n",
    "title VARCHAR,\n",
    "if_latest FLOAT,\n",
    "PRIMARY KEY (journal_id) \n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "create_tables = [authorship_create, article_category_create, category_create, article_create, author_create, journal_create]\n",
    "\n",
    "for query in create_tables:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca09335-0049-41da-96a4-bdb0fb7a9c3d",
   "metadata": {},
   "source": [
    "# Insert into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3e6e-2ba3-41cf-ae0f-3a7ab42d96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorship_insert = (\"\"\"\n",
    "INSERT INTO authorship (article_id, author_id)\n",
    "VALUES (%s, %s)\n",
    "\"\"\") ## ON CONFLICT (article_id) DO NOTHING <-- might need to add to the end\n",
    "\n",
    "article_category_insert = (\"\"\"\n",
    "INSERT INTO article_category (article_id, category_id)\n",
    "VALUES (%s, %s)\n",
    "\"\"\")\n",
    "\n",
    "category_insert = (\"\"\"\n",
    "INSERT INTO category (category_id, superdom, subdom)\n",
    "VALUES (%s, %s, %s)\n",
    "\"\"\")\n",
    "\n",
    "article_insert = (\"\"\"\n",
    "INSERT INTO article (article_id, title, doi, n_authors, journal_issn, n_cites, year)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\")\n",
    "\n",
    "author_insert = (\"\"\"\n",
    "INSERT INTO author (author_id, last_name, first_name, middle_name, gender, affiliation, hindex)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\")\n",
    "\n",
    "journal_insert = (\"\"\"\n",
    "INSERT INTO journal (journal_id, issn, title, if_latest)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\")\n",
    "\n",
    "# ----- #\n",
    "\n",
    "# Name of tables (for later print)\n",
    "tables = [authorship, article_category, category, article, author, journal]\n",
    "authorship.name = 'authorship'\n",
    "article_category.name = 'article_category'\n",
    "category.name = 'category'\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "journal.name = 'journal'\n",
    "\n",
    "insert_tables = [authorship_insert, article_category_insert, category_insert, article_insert, author_insert, journal_insert]\n",
    "\n",
    "\n",
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Trying to insert table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()\n",
    "        \n",
    "for  i in range(len(tables)):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6235b-b285-474c-9596-d006feca6061",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c3f7c-126f-4d96-b37a-5631f0257126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ipython-sql\n",
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530a291-0ebd-4af0-a999-bc1337caf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a5d25-1d54-42df-b237-9eec0fad2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653ec78-1f13-406e-b64a-08482bac57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a5deb-968f-49af-902f-d7ab8d5fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM author LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbef25-7b1b-4030-bc61-bab4ac2dec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78fe71",
   "metadata": {},
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097504f",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18104987",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce61fb1",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255073e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d1c7378",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
