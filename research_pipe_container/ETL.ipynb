{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88108145",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniv√§li\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f3067",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "from math import floor\n",
    "import time\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "from unidecode import unidecode # international encoding fo names\n",
    "\n",
    "### Custom Scripts (ETL, augmentations, SQL)\n",
    "#from dags.scripts.raw_to_tables import *\n",
    "#from dags.scripts.augmentations import *\n",
    "#from dags.scripts.final_tables import *\n",
    "from dags.scripts.sql_queries import *\n",
    "from dags.scripts.neo4j_queries import *\n",
    "\n",
    "### Database drivers\n",
    "import psycopg2\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786d3fd",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d9e7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tables_or_ingest_raw():\n",
    "    if os.path.exists('dags/tables') and len(os.listdir('dags/tables')) == 8: # directory + 7 tables\n",
    "        print('Tables exist...')\n",
    "        author = pd.read_csv('dags/tables/author.csv')\n",
    "        authorshiphip = pd.read_csv('dags/tables/authorship.csv')\n",
    "        article = pd.read_csv('dags/tables/article.csv')\n",
    "        article_category = pd.read_csv('dags/tables/article_category.csv')\n",
    "        category = pd.read_csv('dags/tables/category.csv')\n",
    "        journal = pd.read_csv('dags/tables/journal.csv')\n",
    "        print('Tables are in the working directory!')\n",
    "\n",
    "    ## If tables do not exist, pull from kaggle (or local machine), proprocess to tables\n",
    "    else: \n",
    "        print('Preparing tables...')\n",
    "        print()\n",
    "        ingest_and_prepare()\n",
    "        print('Tables are in the working directory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5e1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables exist...\n",
      "Tables are in the working directory!\n"
     ]
    }
   ],
   "source": [
    "find_tables_or_ingest_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd2b40",
   "metadata": {},
   "source": [
    "# 2. Loading Clean Data (or Running Additional Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671a5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_augment():\n",
    "    \"\"\"Function to either check if clean tables exist\n",
    "    or clean the data and write them to .csv\n",
    "    \"\"\"\n",
    "    article = article_ready()\n",
    "    journal = journal_ready()\n",
    "\n",
    "    # Remove not found journals from articles\n",
    "    article = article[article['journal_issn'].isin(journal['journal_issn'])].reset_index(drop = True)\n",
    "    # Update 'article.csv' in 'data_ready' directory\n",
    "    article.to_csv('dags/data_ready/article.csv', index = False)\n",
    "\n",
    "    authorship = authorship_ready(article)\n",
    "    author = author_ready(article, authorship)\n",
    "    article_category = article_category_ready(article)\n",
    "    category = category_ready(article_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f6d9b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_ready' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_or_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mcheck_or_augment\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_or_augment\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Function to either check if clean tables exist\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    or clean the data and write them to .csv\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     article \u001b[38;5;241m=\u001b[39m \u001b[43marticle_ready\u001b[49m()\n\u001b[1;32m      6\u001b[0m     journal \u001b[38;5;241m=\u001b[39m journal_ready()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Remove not found journals from articles\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article_ready' is not defined"
     ]
    }
   ],
   "source": [
    "check_or_augment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5d241",
   "metadata": {},
   "source": [
    "### Author update and augments\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe2efa",
   "metadata": {},
   "source": [
    "### Journal\n",
    "In order to get the journal information, we need the journal ISSN list from the `article` table. Although journal Impact Factor are more common metrics, they are trademarked and, hence, retrieving them is not open-source. The alternative is to use SNIP - source-normalized impact per publication. This is the average number of citations per publication, corrected for differences in citation practice between research domains. Fortunately, the list of journals and their SNIP is available from the CWTS website (https://www.journalindicators.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47394d",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into tables (helper function)\n",
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0959ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_to_dwh():\n",
    "    # Import the data\n",
    "    try:\n",
    "        article = pd.read_csv('dags/data_ready/article.csv')\n",
    "        author = pd.read_csv('dags/data_ready/author.csv')\n",
    "        authorship = pd.read_csv('dags/data_ready/authorship.csv')\n",
    "        category = pd.read_csv('dags/data_ready/category.csv')\n",
    "        article_category = pd.read_csv('dags/data_ready/article_category.csv')\n",
    "        journal = pd.read_csv('dags/data_ready/journal.csv')\n",
    "        tables = [article, author, authorship, category, article_category, journal]\n",
    "\n",
    "        # Name of tables (for later print)\n",
    "        article.name = 'article'\n",
    "        author.name = 'author'\n",
    "        authorship.name = 'authorship'\n",
    "        category.name = 'category'\n",
    "        article_category.name = 'article_category'\n",
    "        journal.name = 'journal'\n",
    "        print(article.head(2))\n",
    "        print(author.head(2))\n",
    "        print(authorship.head(2))\n",
    "        print(category.head(2))\n",
    "        print(article_category.head(2))\n",
    "        print(journal.head(2))\n",
    "        print('All tables inserted to DWH.')\n",
    "    except:\n",
    "        print('Error with importing the data tables')\n",
    "       \n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(host=\"postgres\", user=\"airflow\", password=\"airflow\", database =\"airflow\", port = 5432)\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # create sparkify database with UTF8 encoding\n",
    "    cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "    cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")\n",
    "\n",
    "    # Drop Tables \n",
    "    try: \n",
    "        for query in drop_tables:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print('All tables dropped.')\n",
    "    except:\n",
    "        print('Error with dropping tables.')\n",
    "        \n",
    "    # Create Tables\n",
    "    try: \n",
    "        for query in create_tables:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        print('All tables created.')\n",
    "    except:\n",
    "        print('Error with creating tables.')\n",
    "\n",
    "    # Insert into tables\n",
    "    for i in tqdm(range(len(tables))):\n",
    "        insert_to_tables(cur, tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdeb0c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Warehouse Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729006e8",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6cbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%sql postgresql://airflow:airflow@postgres/airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67741ef",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fea373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://airflow:***@postgres/airflow\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>54128</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(54128,)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*) FROM authorship COUNT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f940b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b11a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd04701",
   "metadata": {},
   "outputs": [],
   "source": [
    "author = 'WangX'\n",
    "# Get the articles\n",
    "papers = authorship[authorship['author_id'] == author]['article_id'].values\n",
    "\n",
    "# Get all authors\n",
    "co_authors = authorship[authorship['article_id'].isin(papers)]\n",
    "\n",
    "# N pubs with unique co-authors\n",
    "npubs_coauthors = co_authors[co_authors['author_id'] != author].groupby(['author_id']).size()\n",
    "\n",
    "# n Cites with unique co-authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c4923",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>\n",
    "\n",
    "Tutorial: https://www.youtube.com/watch?v=PfySvVqHAWo&t=33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac91e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_neo = Neo4jConnection(uri='bolt://neo:7687', user='', pwd='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all nodes\n",
    "# conn.query('MATCH (a) DELETE a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e157e0f",
   "metadata": {},
   "source": [
    "### Add constraints to ID variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71402d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pandas_to_neo():\n",
    "    # Import the data\n",
    "    try:\n",
    "        article = pd.read_csv('dags/data_ready/article.csv')\n",
    "        author = pd.read_csv('dags/data_ready/author.csv')\n",
    "        authorship = pd.read_csv('dags/data_ready/authorship.csv')\n",
    "        category = pd.read_csv('dags/data_ready/category.csv')\n",
    "        article_category = pd.read_csv('dags/data_ready/article_category.csv')\n",
    "        journal = pd.read_csv('dags/data_ready/journal.csv')\n",
    "        tables = [article, author, authorship, category, article_category, journal]\n",
    "\n",
    "        # Name of tables (for later print)\n",
    "        article.name = 'article'\n",
    "        author.name = 'author'\n",
    "        authorship.name = 'authorship'\n",
    "        category.name = 'category'\n",
    "        article_category.name = 'article_category'\n",
    "        journal.name = 'journal'\n",
    "        print(article.head(2))\n",
    "        print(author.head(2))\n",
    "        print(authorship.head(2))\n",
    "        print(category.head(2))\n",
    "        print(article_category.head(2))\n",
    "        print(journal.head(2))\n",
    "        print('All tables staged for Neo4J.')\n",
    "    except:\n",
    "        print('Error with importing the data tables.')\n",
    "\n",
    "    # Neo4J Connection\n",
    "    conn_neo = Neo4jConnection(uri='bolt://neo:7687', user='', pwd='')\n",
    "\n",
    "    # Add ID uniqueness constraint to optimize queries\n",
    "    conn_neo.query('CREATE CONSTRAINT ON(n:Category) ASSERT n.id IS UNIQUE')\n",
    "    conn_neo.query('CREATE CONSTRAINT ON(j:Journal) ASSERT j.id IS UNIQUE')\n",
    "    conn_neo.query('CREATE CONSTRAINT ON(au:Author) ASSERT au.id IS UNIQUE')\n",
    "    conn_neo.query('CREATE CONSTRAINT ON(ar:Article) ASSERT ar.id IS UNIQUE')\n",
    "   \n",
    "    print(f'Inserting pandas to NEO4J...')\n",
    "    try:\n",
    "        add_category(conn_neo, category)\n",
    "        add_journal(conn_neo, journal)\n",
    "        add_author(conn_neo, author)\n",
    "        add_article(conn_neo, article) \n",
    "        add_article_category(conn_neo, article_category)\n",
    "        add_authorship(conn_neo, authorship)\n",
    "        print(f'pandas to Neo4J inserted!')\n",
    "    except:\n",
    "        print('Error or entities already exist (check the subsequent info)!')\n",
    "        print('Below are the counts of entities in the Neo4J database (must be non-null):')\n",
    "        n_articles = conn_neo.query('MATCH (n:Article) RETURN COUNT(n) AS ct')\n",
    "        n_authors = conn_neo.query('MATCH (n:Author) RETURN COUNT(n) AS ct')\n",
    "        n_journals = conn_neo.query('MATCH (n:Journal) RETURN COUNT(n) AS ct')   \n",
    "        n_categories =  conn_neo.query('MATCH (n:Category) RETURN COUNT(n) AS ct')  \n",
    "        \n",
    "        print(f\"Number of articles in the NEO4J database: {n_articles[0]['ct']}\")\n",
    "        print(f\"Number of authors in the NEO4J database: {n_authors[0]['ct']}\")\n",
    "        print(f\"Number of journals in the NEO4J database: {n_journals[0]['ct']}\")\n",
    "        print(f\"Number of categories in the NEO4J database: {n_categories[0]['ct']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b485cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conn_neo.query('MATCH (n:Article) RETURN COUNT(n) AS ct')\n",
    "print(result[0]['ct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = conn_neo.query('MATCH (n:Author) RETURN COUNT(n) AS ct')\n",
    "print(result[0]['ct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ae7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7927bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87da74f2",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e504e5",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86628de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b71a5a",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = conn_neo.query(\n",
    "\"\"\"\n",
    "MATCH (author:Author)-[:AUTHORED]->(article:Article) \n",
    "WHERE author.id = \"GousiosG\" \n",
    "WITH author, COUNT(article) AS number_of_articles, collect(article) AS articles\n",
    "ORDER BY number_of_articles DESC \n",
    "UNWIND articles AS article\n",
    "MATCH (coauthor:Author)-[:AUTHORED]->(article)\n",
    "RETURN article, collect(coauthor), COUNT(article), COUNT(coauthor)\n",
    "\"\"\"\n",
    ")\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ad616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego-network WITH the author\n",
    "MATCH (author:Author)-[:AUTHORED]->(article:Article) \n",
    "WHERE author.id = \"GousiosG\" # add specific name\n",
    "WITH author, COUNT(article) AS number_of_articles, collect(article) AS articles\n",
    "ORDER BY number_of_articles DESC \n",
    "LIMIT 1\n",
    "UNWIND articles AS article\n",
    "MATCH (coauthor:Author)-[:AUTHORED]->(article)\n",
    "RETURN article, collect(coauthor), COUNT(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198abac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego-network WITHOUT the author\n",
    "# https://stackoverflow.com/questions/28816222/finding-a-list-of-neo4j-nodes-which-have-the-most-relationships-back-to-another\n",
    "MATCH (author:Author)-[:AUTHORED]->(article:Article) \n",
    "WITH author, COUNT(article) AS number_of_articles, collect(article) AS articles\n",
    "ORDER BY number_of_articles DESC \n",
    "LIMIT 1\n",
    "UNWIND articles AS article\n",
    "MATCH (coauthor:Author)-[:AUTHORED]->(article)\n",
    "WHERE coauthor <> author\n",
    "RETURN article, collect(coauthor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37bd99",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
