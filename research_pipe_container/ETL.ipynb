{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d3cee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a4a43",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ff1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install psycopg2 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1d7d8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operationsõ\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "from math import floor\n",
    "import time\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "from unidecode import unidecode # international encoding fo names\n",
    "\n",
    "### Custom Scripts (ETL, augmentations, SQL)\n",
    "from scripts.raw_to_tables import *\n",
    "from scripts.augmentations import *\n",
    "from scripts.sql_queries import *\n",
    "\n",
    "#import psycopg2\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ae6ec",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0149638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tables...\n",
      "\n",
      "Time of pipeline start: Tue Dec 27 14:27:43 2022\n",
      "\n",
      "Skipping, found downloaded files in \"./arxiv\" (use force=True to force download)\n",
      "Dimensions of the df with valid DOIs: (1088470, 6)\n",
      "Dimensions of the df with dropped duplicates: (1088468, 6)\n",
      "Dimensions of the df with short titles removed: (79958, 6)\n",
      "Dimensions of the df with only CS papers: (79958, 6)\n",
      "\n",
      "Data Ingestion Time elapsed: 39.64675688743591 seconds.\n",
      "Memory usage of raw df: 0.03783251345157623 GB.\n",
      "Directory 'tables' exists.\n",
      "Writing pandas tables to .csv-files.\n",
      "Pandas tables to .csv-files successfully written!\n",
      "\n",
      "ETL Runtime: 49.638529 sec.\n",
      "Tables are in the working directory!\n"
     ]
    }
   ],
   "source": [
    "# First check if the tables are already in the system\n",
    "## If tables exist, import from .csv\n",
    "\n",
    "if os.path.exists('./tables') and len(os.listdir('./tables')) == 7: # directory + 6 tables\n",
    "    print('Tables exist...')\n",
    "    author = pd.read_csv('./tables/author.csv')\n",
    "    authorshiphip = pd.read_csv('./tables/authorship.csv')\n",
    "    article = pd.read_csv('./tables/article.csv')\n",
    "    article_category = pd.read_csv('./tables/article_category.csv')\n",
    "    category = pd.read_csv('./tables/category.csv')\n",
    "    journal = pd.read_csv('./tables/journal.csv')\n",
    "    print('Tables are in the working directory!')\n",
    "    \n",
    "## If tables do not exist, pull from kaggle (or local machine), proprocess to tables\n",
    "else: \n",
    "    print('Preparing tables...')\n",
    "    print()\n",
    "    ingest_and_prepare()\n",
    "    print('Tables are in the working directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c1018",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender and affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470297bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Article\n",
    "In this section, we use the `requests` library to fetch the citation based onthe Crossref URL of the work's DOI. We have found that this method is faster than querying the Crossref API. We extract the work type and the number of citations that the work has received; additionally, the journal ISSN for the publication is retrieved if it is available.\n",
    "\n",
    "We want to note that although we initially also wanted to fetch author affiliation, it is not really feasible, as most of this information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfcd634e-b941-466f-91fb-1ef4385761b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB! Must follow the sequence below\n",
    "\n",
    "# Check if clean 'article' table exists\n",
    "if os.path.exists('./data_ready/article.csv'):\n",
    "    article = pd.read_csv('./data_ready/article.csv')\n",
    "else:\n",
    "    # Check if augments already done\n",
    "    if os.path.exists('./tables/article_augmented_raw.csv'):\n",
    "        article = pd.read_csv('./tables/article_augmented_raw.csv')\n",
    "    else:\n",
    "        # Candidate for citation updating!!\n",
    "        article = pd.read_csv('./tables/article.csv')\n",
    "\n",
    "        # Use Crossref API for extracting cites, paper type, and journal ISSNs\n",
    "        batches = range(0, len(article), 2000)\n",
    "        for b in batches:\n",
    "            start_range = b\n",
    "            end_range = b + 2000\n",
    "            # Use the custom augmentation script\n",
    "            ## NB! 5k records in appx 30min, 2k records in appx 14min \n",
    "            fetch_article_augments(start_range, end_range)\n",
    "        # Last batch\n",
    "        print(time.ctime())\n",
    "        start_article = time.time()\n",
    "        start_range = batches[-1]\n",
    "        end_range = len(article)\n",
    "        fetch_article_augments(start_range, end_range)\n",
    "        end_article = time.time()\n",
    "        end_article - start_article/60\n",
    "        end_batches = time.time()\n",
    "        print(f'End of article augmentation: {end_batches}')\n",
    "\n",
    "        # Write to a separate csv (without filtering\n",
    "        article.to_csv('tables/article_augmented_raw.csv', index = False)\n",
    "    \n",
    "    # Include only journal articles\n",
    "    article_journal = article[article['type'] == 'journal-article'].reset_index(drop = True)\n",
    "    \n",
    "    # Write to 'data_ready' directory\n",
    "    article_journal.to_csv('./data_ready/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639e422-cf04-4dba-a8b4-a779df63a343",
   "metadata": {},
   "source": [
    "## Journal (and article update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e89805-3ede-4e39-87cc-6c0af15aa8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing CWTS data (2021)...\n",
      "CWTS (2021) data imported!\n",
      "The number of unique journals: 2219\n",
      "Matching journal ISSNs with names and SNIPs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2219/2219 [00:07<00:00, 305.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ISSNs with missing data...\n",
      "Writing a clean journal.csv\n",
      "'journal.csv' written to 'data_ready' directory!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./data_ready/journal.csv'):\n",
    "    journal = pd.read_csv('./data_ready/journal.csv')\n",
    "else:\n",
    "    journal = pd.read_csv('tables/journal.csv')\n",
    "    \n",
    "    # Import the journal database data\n",
    "    ## NB! It may take some time\n",
    "    print('Importing CWTS data (2021)...')\n",
    "    cwts_data = pd.read_excel('augmentation/CWTS Journal Indicators April 2022.xlsx',\n",
    "                         sheet_name = 'Sources')\n",
    "    # Fix colnames (replace spaces and lower)\n",
    "    cwts_data.columns = [col.replace(' ','_').lower() for col in cwts_data.columns] \n",
    "    # Include only 2021 records\n",
    "    cwts21 = cwts_data[cwts_data['year'].isin([2021])].reset_index(drop = True)\n",
    "    print('CWTS (2021) data imported!')\n",
    "    \n",
    "    # Find the journals\n",
    "    journal['journal_issn'] = article['journal_issn'].unique() # NB!! 'article' must be in pwd\n",
    "    journal = journal[~journal['journal_issn'].isnull()] # remove NAs\n",
    "    journal = journal.sort_values('journal_issn').reset_index(drop = True)\n",
    "    \n",
    "    print(f'The number of unique journals: {len(journal)}')\n",
    "    journal = find_journal_stats(journal, cwts21) # from augmentations.py\n",
    "    \n",
    "    print('Writing a clean journal.csv')\n",
    "    journal.to_csv('./data_ready/journal.csv', index = False)\n",
    "    print(\"'journal.csv' written to 'data_ready' directory!\")\n",
    "\n",
    "# Remove not found journals from articles\n",
    "article = article[article['journal_issn'].isin(journal['journal_issn'])].reset_index(drop = True)\n",
    "# Update 'article.csv' in 'data_ready' directory\n",
    "article.to_csv('./data_ready/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a9504-ff58-4b81-8c32-cb63df7d57ce",
   "metadata": {},
   "source": [
    "### Authorship update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f184519-96ac-40f7-8ace-2dd84e1f577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/authorship.csv'):\n",
    "    authorship = pd.read_csv('./data_ready/authorship.csv')\n",
    "else:\n",
    "    authorship = pd.read_csv('./tables/authorship.csv')\n",
    "    # Include only the relations in 'article' table\n",
    "    authorship = authorship[authorship['article_id'].isin(article['article_id'])].sort_values('article_id').reset_index(drop = True)\n",
    "    # Write to csv\n",
    "    authorship.to_csv('./data_ready/authorship.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652d144-011d-490d-aa10-764d1dd61325",
   "metadata": {},
   "source": [
    "### Author update and augments\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8d7f1a10-106d-4dcc-bd45-7551677d6dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/author.csv'):\n",
    "    author = pd.read_csv('./data_ready/author.csv')\n",
    "else:\n",
    "    author = pd.read_csv('./tables/author.csv').drop_duplicates()\n",
    "    \n",
    "    # Filter authors\n",
    "    author = author[author['author_id'].isin(authorship['author_id'])].drop_duplicates(['author_id']).reset_index(drop = True)\n",
    "    \n",
    "    # Augment gender\n",
    "    print('Importing gender information...')\n",
    "    names_genders = pd.read_csv('./augmentation/names_genders.csv')[['first_name', 'gender']]\n",
    "    author = author.merge(names_genders, on = 'first_name', how = 'left')\n",
    "    print('Gender augmentation done where possible')\n",
    "    \n",
    "    # Number of publications\n",
    "    npubs = pd.DataFrame(authorship.reset_index(drop = True).groupby('author_id').size()).sort_values('author_id').reset_index()\n",
    "    npubs.columns = ['author_id', 'total_pubs']\n",
    "    author = author.merge(npubs, on = 'author_id')\n",
    "    \n",
    "    # Additional augments\n",
    "    ## Statistics table\n",
    "    stats = authorship.merge(article[['article_id', 'n_cites', 'n_authors']], on = 'article_id').sort_values('author_id').reset_index(drop = True)\n",
    "\n",
    "    ## Add new columns to author table\n",
    "    author['total_cites'] = np.zeros(len(author))\n",
    "    author['avg_cites'] = np.zeros(len(author))\n",
    "    author['med_coauthors'] = np.zeros(len(author))\n",
    "    author['hindex'] = np.zeros(len(author))\n",
    "    \n",
    "    ## Add statistics to authors\n",
    "    ### NB! Slow run...\n",
    "    print('Computing author statistics...')\n",
    "    for i in tqdm(range(len(author))):    \n",
    "        author_id = author.loc[i, 'author_id']\n",
    "        papers = stats[stats['author_id'] == author_id].sort_values('n_cites').reset_index(drop = True)\n",
    "        citations = papers['n_cites'].sort_values(ascending = False).reset_index(drop = True)\n",
    "\n",
    "            # Stats\n",
    "        author.loc[i, 'total_cites'] = papers['n_cites'].sum() # Total number of citations\n",
    "        author.loc[i, 'avg_cites'] = round(author.loc[i, 'total_cites']/len(papers),3) # Average number of citations per paper\n",
    "        author.loc[i, 'med_coauthors'] = np.median(papers['n_authors']-1) # subtract oneself\n",
    "\n",
    "            # h-index\n",
    "        author.loc[i, 'hindex'] = hindex(citations, len(citations))\n",
    "    \n",
    "    print('Computing done!') \n",
    "    print('Saving author table to .csv...') \n",
    "    \n",
    "    # Save to csv\n",
    "    author.to_csv('./data_ready/author.csv', index = False)\n",
    "    print('author table saved!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d8f9a6eb-bcd6-4450-9cf6-dc34854114df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>total_pubs</th>\n",
       "      <th>total_cites</th>\n",
       "      <th>avg_cites</th>\n",
       "      <th>med_coauthors</th>\n",
       "      <th>hindex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201819N</td>\n",
       "      <td>201819</td>\n",
       "      <td>Networks</td>\n",
       "      <td>Computer</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADNIt</td>\n",
       "      <td>ADNI</td>\n",
       "      <td>the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AISahafH</td>\n",
       "      <td>AISahaf</td>\n",
       "      <td>Harith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALTamF</td>\n",
       "      <td>ALTam</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWANOT</td>\n",
       "      <td>AWANO</td>\n",
       "      <td>Tomoharu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56197</th>\n",
       "      <td>vonWaldthausenL</td>\n",
       "      <td>vonWaldthausen</td>\n",
       "      <td>Leopold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56198</th>\n",
       "      <td>vonWangenheimA</td>\n",
       "      <td>vonWangenheim</td>\n",
       "      <td>Aldo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56199</th>\n",
       "      <td>vonWurstembergerP</td>\n",
       "      <td>vonWurstemberger</td>\n",
       "      <td>Philippe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56200</th>\n",
       "      <td>vonderOheU</td>\n",
       "      <td>vonderOhe</td>\n",
       "      <td>Ulrich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56201</th>\n",
       "      <td>zengK</td>\n",
       "      <td>zeng</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56202 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_id         last_name first_name middle_name gender  \\\n",
       "0                201819N            201819   Networks    Computer      M   \n",
       "1                  ADNIt              ADNI        the         NaN    NaN   \n",
       "2               AISahafH           AISahaf     Harith         NaN      M   \n",
       "3                 ALTamF             ALTam          F         NaN    NaN   \n",
       "4                 AWANOT             AWANO   Tomoharu         NaN      M   \n",
       "...                  ...               ...        ...         ...    ...   \n",
       "56197    vonWaldthausenL    vonWaldthausen    Leopold         NaN      M   \n",
       "56198     vonWangenheimA     vonWangenheim       Aldo         NaN    NaN   \n",
       "56199  vonWurstembergerP  vonWurstemberger   Philippe         NaN      M   \n",
       "56200         vonderOheU         vonderOhe     Ulrich         NaN      M   \n",
       "56201              zengK              zeng          K         NaN    NaN   \n",
       "\n",
       "       total_pubs  total_cites  avg_cites  med_coauthors  hindex  \n",
       "0               1          0.0      0.000            6.0     0.0  \n",
       "1               1          4.0      4.000            7.0     1.0  \n",
       "2               1          5.0      5.000            5.0     1.0  \n",
       "3               1         29.0     29.000            2.0     1.0  \n",
       "4               1          9.0      9.000            4.0     1.0  \n",
       "...           ...          ...        ...            ...     ...  \n",
       "56197           1          7.0      7.000            7.0     1.0  \n",
       "56198           3         19.0      6.333            1.0     2.0  \n",
       "56199           2         26.0     13.000            4.0     2.0  \n",
       "56200           1          2.0      2.000            2.0     1.0  \n",
       "56201           1          3.0      3.000            2.0     1.0  \n",
       "\n",
       "[56202 rows x 10 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457a68a-fd9e-41d2-90e9-e3e7eff914ff",
   "metadata": {},
   "source": [
    "### Article category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9531f7cc-1605-4f5e-8c84-7f0af73dcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/article_category.csv'):\n",
    "    article_category = pd.read_csv('./data_ready/article_category.csv')\n",
    "else:\n",
    "    article_category = pd.read_csv('./tables/article_category.csv')\n",
    "    article_category = article_category[article_category['article_id'].isin(article['article_id'])].reset_index(drop = True)\n",
    "    article_category.to_csv('./data_ready/article_category.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99ae7f-97f2-4a57-a335-1258025cee3a",
   "metadata": {},
   "source": [
    "### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9e764504-f623-4263-8aa3-cc73515e5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/category.csv'):\n",
    "    category = pd.read_csv('./data_ready/category.csv')\n",
    "else:\n",
    "    category = pd.read_csv('./tables/category.csv')\n",
    "    category = category[category['category_id'].isin(article_category['category_id'])].reset_index(drop = True)\n",
    "    category.to_csv('./data_ready/category.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "64b02942-bbb1-4e6e-8ab6-ea5602b8e2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>superdom</th>\n",
       "      <th>subdom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adap-org</td>\n",
       "      <td>adap-org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astro-ph</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astro-ph.CO</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astro-ph.EP</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>EP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>astro-ph.GA</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>stat.CO</td>\n",
       "      <td>stat</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>stat.ME</td>\n",
       "      <td>stat</td>\n",
       "      <td>ME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat</td>\n",
       "      <td>ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>stat.OT</td>\n",
       "      <td>stat</td>\n",
       "      <td>OT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>stat.TH</td>\n",
       "      <td>stat</td>\n",
       "      <td>TH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category_id  superdom subdom\n",
       "0       adap-org  adap-org    NaN\n",
       "1       astro-ph  astro-ph    NaN\n",
       "2    astro-ph.CO  astro-ph     CO\n",
       "3    astro-ph.EP  astro-ph     EP\n",
       "4    astro-ph.GA  astro-ph     GA\n",
       "..           ...       ...    ...\n",
       "131      stat.CO      stat     CO\n",
       "132      stat.ME      stat     ME\n",
       "133      stat.ML      stat     ML\n",
       "134      stat.OT      stat     OT\n",
       "135      stat.TH      stat     TH\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f119a19-764a-424f-9c1f-55c31e47c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b38a71",
   "metadata": {},
   "source": [
    "### Journal\n",
    "In order to get the journal information, we need the journal ISSN list from the `article` table. Although journal Impact Factor are more common metrics, they are trademarked and, hence, retrieving them is not open-source. The alternative is to use SNIP - source-normalized impact per publication. This is the average number of citations per publication, corrected for differences in citation practice between research domains. Fortunately, the list of journals and their SNIP is available from the CWTS website (https://www.journalindicators.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a9219",
   "metadata": {},
   "source": [
    "#### Merge author-names-genders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717ac46",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f216620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "authorship = pd.read_csv('data_ready/authorship.csv')\n",
    "article_category = pd.read_csv('data_ready/article_category.csv')\n",
    "category = pd.read_csv('data_ready/category.csv')\n",
    "article = pd.read_csv('data_ready/article.csv')\n",
    "author = pd.read_csv('data_ready/author.csv')\n",
    "journal = pd.read_csv('data_ready/journal.csv')\n",
    "\n",
    "tables = [authorship, article_category, category, article, author, journal]\n",
    "\n",
    "# Name of tables (for later print)\n",
    "authorship.name = 'authorship'\n",
    "article_category.name = 'article_category'\n",
    "category.name = 'category'\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "journal.name = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348a384",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f097d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3914",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71355b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be56c64",
   "metadata": {},
   "source": [
    "# Drop Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29636aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that a table, e.g., 'jounal', is not in the database\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c1bdd",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea183db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_tables:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the tables (e.g., 'author') are created\n",
    "## Should be empty\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bb504",
   "metadata": {},
   "source": [
    "# Insert into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()\n",
    "        \n",
    "for  i in range(len(tables)):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM author LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943b73f",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39785041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47377a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81867700",
   "metadata": {},
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d3d48",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404aeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919e657b",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140341c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb4b891",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796c291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25eb5423",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45774620",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
