{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02786bb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniv√§li\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d39d3",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install psycopg2 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a15ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "from unidecode import unidecode # international encoding fo names\n",
    "\n",
    "### Custom Scripts (ETL, SQL)\n",
    "from scripts.raw_to_tables import *\n",
    "from scripts.sql_queries import *\n",
    "\n",
    "#import psycopg2\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4046767",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c51f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables exist...\n",
      "Tables in the working directory!\n"
     ]
    }
   ],
   "source": [
    "start_pipe = time.time() # Initialize the time of pipeline\n",
    "\n",
    "# First check if the tables are already in the system\n",
    "## If tables exist, import from .csv\n",
    "\n",
    "if os.path.exists('./tables') and len(os.listdir('./tables')) == 7: # directory + 6 tables\n",
    "    print('Tables exist...')\n",
    "    author = pd.read_csv('tables/author.csv')\n",
    "    authorshiphip = pd.read_csv('tables/authorship.csv')\n",
    "    article = pd.read_csv('tables/article.csv')\n",
    "    article_category = pd.read_csv('tables/article_category.csv')\n",
    "    category = pd.read_csv('tables/category.csv')\n",
    "    journal = pd.read_csv('tables/journal.csv')\n",
    "    print('Tables in the working directory!')\n",
    "    \n",
    "\n",
    "## If tables do not exist, pull from kaggle (or local machine), proprocess to tables\n",
    "else: \n",
    "    \n",
    "    start_etl = time.time() # Initialize the time of ETL\n",
    "    print(f'Time of pipeline start: {time.ctime(start_pipe)}')\n",
    "    print()\n",
    "    # Data ingestion\n",
    "    df = ingest_and_process(force = False)\n",
    "\n",
    "    # Prepare Pandas dataframes\n",
    "    authorship, author = authorship_author_extract(df)\n",
    "    article_category, category = article_category_category_extract(df)\n",
    "    article = article_extract(df)\n",
    "    journal = journal_extract()\n",
    "    \n",
    "    # Clean the data last time: remove all authors with NaNs or too short names\n",
    "    ## NaNs\n",
    "    author = author[~author['author_id'].isnull()]\n",
    "    nan_authors = authorship[authorship['author_id'].isnull()]['article_id'].values\n",
    "    article = article.loc[~article['article_id'].isin(nan_authors)]\n",
    "    authorship = authorship.loc[~authorship['article_id'].isin(nan_authors)]\n",
    "\n",
    "    ## Too short (< 4) names\n",
    "    author = author[~(author['author_id'].str.len() < 4)].reset_index(drop = True)\n",
    "    short_authors = authorship[(authorship['author_id'].str.len() < 4)]['article_id'].values\n",
    "    article = article.loc[~article['article_id'].isin(short_authors)].reset_index(drop = True)\n",
    "    authorship = authorship.loc[~authorship['article_id'].isin(short_authors)].reset_index(drop = True)\n",
    "    \n",
    "    ## Write .csv-s to 'tables' directory\n",
    "    ### Create the 'tables' directory\n",
    "    !mkdir tables\n",
    "    \n",
    "    ### Write the tables as csv\n",
    "    authorship.to_csv('tables/authorship.csv', index = False)\n",
    "    article_category.to_csv('tables/article_category.csv', index = False)\n",
    "    category.to_csv('tables/category.csv', index = False)\n",
    "    journal.to_csv('tables/journal.csv', index = False)\n",
    "    article.to_csv('tables/article.csv', index = False)\n",
    "    author.to_csv('tables/author.csv', index = False)\n",
    "\n",
    "    end_etl = time.time() # Endtime of ETL\n",
    "    print(f'ETL Runtime: {round(end_etl - start_etl, 6)} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41105584",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5843693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender and affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4191a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Article\n",
    "In this section, we use the `requests` library to fetch the citation based onthe Crossref URL of the work's DOI. We have found that this method is faster than querying the Crossref API. We extract the work type and the number of citations that the work has received; additionally, the journal ISSN for the publication is retrieved if it is available.\n",
    "\n",
    "We want to note that although we initially also wanted to fetch author affiliation, it is not really feasible, as most of this information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c1be19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>n_authors</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>type</th>\n",
       "      <th>n_cites</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0046</td>\n",
       "      <td>A limit relation for entropy and channel capac...</td>\n",
       "      <td>10.1063/1.2779138</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0062</td>\n",
       "      <td>On-line Viterbi Algorithm and Its Relationship...</td>\n",
       "      <td>10.1007/978-3-540-74126-8_23</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0098</td>\n",
       "      <td>Sparsely-spread CDMA - a statistical mechanics...</td>\n",
       "      <td>10.1088/1751-8113/40/41/004</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0217</td>\n",
       "      <td>Capacity of a Multiple-Antenna Fading Channel ...</td>\n",
       "      <td>10.1109/TIT.2008.2011437</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0301</td>\n",
       "      <td>Differential Recursion and Differentially Alge...</td>\n",
       "      <td>10.1145/1507244.1507252</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title  \\\n",
       "0   704.0046  A limit relation for entropy and channel capac...   \n",
       "1   704.0062  On-line Viterbi Algorithm and Its Relationship...   \n",
       "2   704.0098  Sparsely-spread CDMA - a statistical mechanics...   \n",
       "3   704.0217  Capacity of a Multiple-Antenna Fading Channel ...   \n",
       "4   704.0301  Differential Recursion and Differentially Alge...   \n",
       "\n",
       "                            doi  n_authors  journal_issn  type  n_cites  year  \n",
       "0             10.1063/1.2779138          3           NaN   NaN      NaN  2009  \n",
       "1  10.1007/978-3-540-74126-8_23          3           NaN   NaN      NaN  2010  \n",
       "2   10.1088/1751-8113/40/41/004          2           NaN   NaN      NaN  2009  \n",
       "3      10.1109/TIT.2008.2011437          2           NaN   NaN      NaN  2010  \n",
       "4       10.1145/1507244.1507252          1           NaN   NaN      NaN  2009  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article = pd.read_csv('tables/article.csv') # import if necessary\n",
    "article.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4352ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate for citation updating!!\n",
    "\n",
    "# Use the base url\n",
    "base_url = 'http://api.crossref.org/works/'\n",
    "\n",
    "def fetch_article_augments(start_range, end_range):\n",
    "    for i in range(start_range, end_range, 1): # don't use tqdm if range specified like that...\n",
    "        try:\n",
    "            # Check if the value in work type is of len 3 ('NaN')\n",
    "            if len(article['type'].astype(str)[i]) == 3:\n",
    "                doi = base_url + article.loc[i, 'doi'] # append the doi for the base URL\n",
    "                rqst = requests.get(doi) # request by URL\n",
    "                qr_result = rqst.json() # get the json\n",
    "\n",
    "                if qr_result['status'] == 'ok': # if request successful, make the queries, update fields\n",
    "                    msg = qr_result['message']\n",
    "                    work_type = msg['type']\n",
    "\n",
    "                    article.loc[i, 'type'] = work_type # add work type\n",
    "                    article.loc[i, 'n_cites'] =  qr_result['message']['is-referenced-by-count'] # add reference count\n",
    "\n",
    "                    try: \n",
    "                        article.loc[i, 'journal_issn'] =  qr_result['message']['ISSN'][0] # add journal ISSN\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "        except:\n",
    "            continue\n",
    "    # Overwrite the csv\n",
    "    article.to_csv('tables/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a31ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 23 15:39:37 2022\n"
     ]
    }
   ],
   "source": [
    "print(time.ctime())\n",
    "start_article = time.time()\n",
    "start_range = 15001\n",
    "end_range = 20000\n",
    "\n",
    "fetch_article_augments(start_range, end_range)\n",
    "\n",
    "end_article = time.time()\n",
    "end_article - start_article/60\n",
    "\n",
    "# 5k records in appx 30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.iloc[19999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a975fa",
   "metadata": {},
   "source": [
    "### Journal\n",
    "In order to get the journal information, we need the journal ISSN list from the `article` table. Although journal Impact Factor are more common metrics, they are trademarked and, hence, retrieving them is not open-source. The alternative is to use SNIP - source-normalized impact per publication. This is the average number of citations per publication, corrected for differences in citation practice between research domains. Fortunately, the list of journals and their SNIP is available from the CWTS website (https://www.journalindicators.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db50c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.44174875418345"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwts_data = pd.read_excel('https://www.journalindicators.com/Content/CWTS%20Journal%20Indicators%20April%202022.xlsx',\n",
    "                         sheet_name = 'Sources')\n",
    "\n",
    "# Filter out the unnecessary stuff \n",
    "---\n",
    "\n",
    "# save as a table to augmentations dir\n",
    "\n",
    "# fetch from augmentations dir\n",
    "\n",
    "# list nique ISSNs to journal table\n",
    "\n",
    "# extract the journal names and SNIPs from swts data\n",
    "\n",
    "# save the journal table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f84d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Author: gender\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible.\n",
    "\n",
    "To that end, we are querying the names via the Genderize.io API. It allows for querying 1500 names per day. We exract the names and probabilities, and update our own data table with these data. We then finally join the table by firstname to include the gender column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the author table\n",
    "author = pd.read_csv('tables/author.csv') \n",
    "author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf335e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique valid first names and create a temporary df with firstname and gender\n",
    "names_genders = pd.DataFrame(np.sort(author[author['first_name'].str.len() > 4]['first_name'].unique()))\n",
    "names_genders.columns = ['first_name']\n",
    "names_genders['alph_value'] = names_genders['first_name'].str.extract('([A-Z]+)') # add a column with first letter\n",
    "names_genders = names_genders.loc[(names_genders['alph_value'].str.len() < 2)].reset_index(drop = True) # remove rows where there's more than one letter\n",
    "names_genders = pd.concat([names_genders,pd.DataFrame(columns=['gender', 'prob'])])\n",
    "names_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_genders_file = pd.read_csv('names_genders.csv')\n",
    "names_genders_file['first_name'] = names_genders_file['first_name'].apply(unidecode)\n",
    "names_genders_file = names_genders_file.drop_duplicates(['first_name', 'gender'])\n",
    "names_genders_file\n",
    "names_genders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gender_from_data(external_dataset, name_var, gender_var, prob_var = None):\n",
    "    \n",
    "    # Search for names from the UCI name data set\n",
    "    for i in tqdm(range(len(names_genders))):\n",
    "\n",
    "        if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "                pass\n",
    "        else:\n",
    "            # Extract the name and letter\n",
    "            firstname = names_genders.loc[i, 'first_name']\n",
    "\n",
    "            # Search in a subset of the externalm dataset\n",
    "            idx = external_dataset[external_dataset[name_var] == firstname].index\n",
    "\n",
    "            # If no index found, no name -> do nothing (augment later with API)\n",
    "            if len(idx) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                # If there is no gender data, do nothing\n",
    "                if len(external_dataset.loc[idx, gender_var]) == 0:\n",
    "                    \n",
    "                    if prob_var == None:\n",
    "                        pass\n",
    "                    # If prob-var is provided, update the prob var\n",
    "                    else:\n",
    "                        names_genders.loc[i, 'prob'] = external_dataset.loc[idx, prob_var] # get the gender\n",
    "                else:\n",
    "                    idx = idx.values[0]\n",
    "                    names_genders.loc[i, 'gender'] = external_dataset.loc[idx, gender_var] # get the gender\n",
    "                    names_genders.loc[i, 'prob'] = 1 # set prob to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe059875",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gender_from_data(names_genders_file, 'first_name', 'gender', 'prob')\n",
    "names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI dataset\n",
    "uci = pd.read_csv('uci_name_gender_dataset.csv')[['Name', 'Gender']].sort_values('Name').reset_index(drop = True)\n",
    "uci = uci[~(uci['Name'].str.len() < 3)].reset_index(drop = True) # remove too short names\n",
    "uci['alph_value'] = uci['Name'].str.extract('([A-Z]+)') # create a column for partialling\n",
    "uci = uci.loc[(uci['alph_value'].str.len() < 2)] # remove rows where there's more than one letter\n",
    "uci['Name'] = uci['Name'].apply(unidecode)\n",
    "uci['Name'] = uci['Name'].str.replace('[^a-zA-Z0-9]', '', regex=True).str.strip()\n",
    "uci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ead498",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gender_from_data(uci, 'Name', 'Gender')\n",
    "names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd137a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_names_table(start_n, names_genders):\n",
    "    \n",
    "    # For loop querying the genderize.io API\n",
    "    for i in tqdm(range(start_n, len(names_genders), 1)):\n",
    "        # Extract the name\n",
    "        first_name = names_genders.loc[i, 'first_name'] # first name\n",
    "        # Check if the name has already been checked\n",
    "        ## Query only if the name hasn't been checked already\n",
    "        if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "            pass\n",
    "        else:\n",
    "            try: \n",
    "                gender_info = Genderize().get([first_name])\n",
    "                names_genders.loc[i, 'gender'] = gender_info[0]['gender']\n",
    "                names_genders.loc[i,'prob'] = gender_info[0]['probability']\n",
    "            except:\n",
    "                print(f'Iteration nr {i}')\n",
    "                print('Limit likely exceeded.')\n",
    "                break\n",
    "            finally:\n",
    "                # Write to csv once no more pulls are possible\n",
    "                names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_names_table(3106, names_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of yet not checked names: {names_genders['gender'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29182b33",
   "metadata": {},
   "source": [
    "#### Merge author-names-genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gender table\n",
    "names_genders = pd.read_csv('names_genders.csv')\n",
    "# Exclude the names that were not found\n",
    "found_names = names_genders[names_genders['prob']>0]\n",
    "# Gender values to 'M' and 'F'\n",
    "found_names['gender'] = found_names['gender'].replace(to_replace=['male','female'], value=['M', 'F'])\n",
    "\n",
    "author = author.merge(found_names[['first_name', 'gender']], on = ['first_name'], how = 'right')\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4f470",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "authorship = pd.read_csv('tables/authorship.csv')\n",
    "article_category = pd.read_csv('tables/article_category.csv')\n",
    "category = pd.read_csv('tables/category.csv')\n",
    "article = pd.read_csv('tables/article.csv')\n",
    "author = pd.read_csv('tables/author.csv')\n",
    "journal = pd.read_csv('tables/journal.csv')\n",
    "\n",
    "tables = [authorship, article_category, category, article, author, journal]\n",
    "\n",
    "# Name of tables (for later print)\n",
    "authorship.name = 'authorship'\n",
    "article_category.name = 'article_category'\n",
    "category.name = 'category'\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "journal.name = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fe3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fb0fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf55a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f57d1",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c25fe8",
   "metadata": {},
   "source": [
    "# Drop Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba97e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that a table, e.g., 'jounal', is not in the database\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21365767",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcff1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_tables:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the tables (e.g., 'author') are created\n",
    "## Should be empty\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff69e0",
   "metadata": {},
   "source": [
    "# Insert into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()\n",
    "        \n",
    "for  i in range(len(tables)):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM author LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe00481",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04953a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb74642",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782076de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4801ba5",
   "metadata": {},
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d057c43",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4c57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb145545",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8ec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea05acbe",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8357d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "122377af",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
