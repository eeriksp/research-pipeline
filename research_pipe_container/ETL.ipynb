{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278abcca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb379",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7a8f6-e274-4dbb-91c7-bf62147cda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install psycopg2 -y\n",
    "!pip install pybliometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ffff7-441d-4e71-b74f-86fee3f5e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83fdb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "\n",
    "\n",
    "from scripts.raw_to_tables import *\n",
    "from scripts.sql_queries import *\n",
    "\n",
    "#import psycopg2\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df85880-9903-400e-bd7d-c89cd75e2119",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ac04cf-8325-4d56-97e6-2cb624f4e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of pipeline start: Thu Dec 22 16:38:15 2022\n",
      "\n",
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: qetdrteq\n",
      "Your Kaggle Key: ········\n",
      "Downloading arxiv.zip to ./arxiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1.11G/1.11G [01:37<00:00, 12.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of the df with valid DOIs: (1088470, 6)\n",
      "Dimensions of the df with dropped duplicates: (1088468, 6)\n",
      "Dimensions of the df with short titles removed: (79958, 6)\n",
      "Dimensions of the df with only CS papers: (79958, 6)\n",
      "\n",
      "Data Ingestion Time elapsed: 387.64597606658936 seconds.\n",
      "Memory usage of raw df: 0.03783251345157623 GB.\n",
      "ETL Runtime: 394.590855 sec.\n"
     ]
    }
   ],
   "source": [
    "start_pipe = time.time() # Initialize the time of pipeline\n",
    "start_etl = time.time() # Initialize the time of ETL\n",
    "print(f'Time of pipeline start: {time.ctime(start_pipe)}')\n",
    "print()\n",
    "\n",
    "# Data ingestion\n",
    "df = ingest_and_process(force = False)\n",
    "\n",
    "# Prepare Pandas dataframes\n",
    "authorship, author = authorship_author_extract(df)\n",
    "article_category, category = article_category_category_extract(df)\n",
    "article = article_extract(df)\n",
    "journal = journal_extract()\n",
    "\n",
    "end_etl = time.time() # Endtime of ETL\n",
    "print(f'ETL Runtime: {round(end_etl - start_etl, 6)} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967cc17-9f00-4fb8-aa39-6dff89782b0b",
   "metadata": {},
   "source": [
    "### Additional data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5c560b-3482-4cc1-ab5b-df6a99d295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data last time: remove all authors with NaNs or too short names\n",
    "## NaNs\n",
    "author = author[~author['author_id'].isnull()]\n",
    "nan_authors = authorship[authorship['author_id'].isnull()]['article_id'].values\n",
    "article = article.loc[~article['article_id'].isin(nan_authors)]\n",
    "authorship = authorship.loc[~authorship['article_id'].isin(nan_authors)]\n",
    "\n",
    "## Too short (< 4) names\n",
    "author = author[~(author['author_id'].str.len() < 4)].reset_index(drop = True)\n",
    "short_authors = authorship[(authorship['author_id'].str.len() < 4)]['article_id'].values\n",
    "article = article.loc[~article['article_id'].isin(short_authors)].reset_index(drop = True)\n",
    "authorship = authorship.loc[~authorship['article_id'].isin(short_authors)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54ba22",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## article_category\n",
    "## category\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender and affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3db63d-f3de-46dc-9519-8e2e14ae3de1",
   "metadata": {},
   "source": [
    "### Gender\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible.\n",
    "\n",
    "To that end, we are querying the names via the Genderize.io API. It allows for querying 1500 names per day. We exract the names and probabilities, and update our own data table with these data. We then finally join the table by firstname to include the gender column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4131d9df-1903-4f49-a1fb-4d13fba1b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique valid first names and create a temporary df with firstname and gender\n",
    "names_genders = pd.DataFrame(np.sort(author[author['first_name'].str.len() > 4]['first_name'].unique()))\n",
    "names_genders.columns = ['first_name']\n",
    "names_genders.head()\n",
    "\n",
    "# Data from names_genders df\n",
    "names_genders_file = pd.read_csv('names_genders.csv')\n",
    "\n",
    "# Merge the dfs\n",
    "names_genders = names_genders.merge(names_genders_file, on = 'first_name', how = 'left')\n",
    "names_genders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2380a01-313c-4c9b-a906-cad26ba7ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_names_table(names_genders):\n",
    "    \n",
    "    # For loop querying the genderize.io API\n",
    "    for i in tqdm(range(len(names_genders))):\n",
    "        # Extract the name\n",
    "        first_name = names_genders.loc[i, 'first_name'] # first name\n",
    "        # Check if the name has already been checked\n",
    "        ## Query only if the name hasn't been checked already\n",
    "        if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "            pass\n",
    "        else:\n",
    "            try: \n",
    "                gender_info = Genderize().get([first_name])\n",
    "                names_genders.loc[i, 'gender'] = gender_info[0]['gender']\n",
    "                names_genders.loc[i,'prob'] = gender_info[0]['probability']\n",
    "            except:\n",
    "                print(f'Iteration nr {i}')\n",
    "                print('Limit likely exceeded.')\n",
    "                break\n",
    "            finally:\n",
    "                # Write to csv\n",
    "                names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aed32330-b058-48fb-aabd-29275368e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▋                                  | 1046/21979 [12:23<4:07:56,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration nr 1046\n",
      "Limit likely exceeded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_names_table(names_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47d3a090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Gholamali</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Omid</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL-Qawasmi</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabhas</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aadarsh</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>António</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Antônio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Anuar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>Anubhab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Anubhav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      first_name gender  prob\n",
       "0     'Gholamali   None   0.0\n",
       "1          'Omid   None   0.0\n",
       "2     AL-Qawasmi   None   0.0\n",
       "3         Aabhas   male   1.0\n",
       "4        Aadarsh   male   1.0\n",
       "...          ...    ...   ...\n",
       "1045     António   male   1.0\n",
       "1046     Antônio    NaN   NaN\n",
       "1047       Anuar    NaN   NaN\n",
       "1048     Anubhab    NaN   NaN\n",
       "1049     Anubhav    NaN   NaN\n",
       "\n",
       "[1050 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_genders[:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bade56-092a-4048-b682-4151d4877e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop querying the genderize.io API\n",
    "for i in tqdm(range(len(names_genders))):\n",
    "    # Extract the name\n",
    "    first_name = names_genders.loc[i, 'first_name'] # first name\n",
    "    \n",
    "    # Check if the name has already been checked\n",
    "    ## Query only if the name hasn't been checked already\n",
    "    if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "        pass\n",
    "#    else:\n",
    "        gender_info = Genderize().get([first_name])\n",
    "        names_genders.loc[i, 'gender'] = gender_info[0]['gender']\n",
    "        names_genders.loc[i,'prob'] = gender_info[0]['probability']\n",
    "\n",
    "# Write to csv\n",
    "# names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f8fe1-1c13-4699-b310-31a05e3d6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gender table\n",
    "names_genders = pd.read_csv('names_genders.csv')\n",
    "# Exclude the names that were not found\n",
    "found_names = names_genders[names_genders['prob']>0]\n",
    "# Gender values to 'M' and 'F'\n",
    "found_names['gender'] = found_names['gender'].replace(to_replace=['male','female'], value=['M', 'F'])\n",
    "\n",
    "author = author.merge(found_names[['first_name', 'gender']], on = ['first_name'], how = 'right')\n",
    "author.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc82ba3-1f8c-4506-95d8-0f6506dc6e2e",
   "metadata": {},
   "source": [
    "### Article\n",
    "This section serves both the augmentation as well as data cleaning function. First, the articles are checked for type: only journal articles are being extracted (later, the records that are not journal articles, alongside with authors, etc, will be deleted). Second, article citation count is extracted. Finally, journal ISSN is extracted. The latter is later used for retrieving journal title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446a5d2-fdec-43e0-9f13-c0af3d1554a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ec401-933a-4dfe-bcbd-f18f95d63e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scientific records\n",
    "DOIs = article['doi']\n",
    "\n",
    "# Initialize Crossref\n",
    "cr = Crossref()\n",
    "\n",
    "for i in tqdm(range(len(article))):\n",
    "    doi = DOIs[i]\n",
    "    ref_obj = cr.works(query= doi)['message']['items'][0]\n",
    "    pub_type = ref_obj['type']\n",
    "  #  print(f'Publication type: {pub_type}')\n",
    "    \n",
    "    if pub_type == 'journal-article':\n",
    "        \n",
    "     #   print(f'Fetching... {doi}')\n",
    "              \n",
    "        article.loc[i, 'n_cites'] = ref_obj['reference-count']\n",
    "        article.loc[i, 'journal_issn'] = ref_obj['ISSN'][0]\n",
    "        \n",
    "    #    print(f'DOI {doi} Fetched!')\n",
    "    #    print()\n",
    "        \n",
    "    else:\n",
    "    #    print('Not a journal article, passed')\n",
    "    #    print()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aeecf8-9f85-4901-8acf-a8aabf273fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d076e-1d62-4faf-b82e-92d95b436810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f78382-4363-448c-802b-150f74f3b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_authorship.iterrows():\n",
    "    #Get row of Article\n",
    "    art_row = article.loc[article[\"article_id\"] == row[\"article_id\"]]\n",
    "    doi = art_row[\"doi\"].iloc[0]\n",
    "    data = get_publication_data(doi)\n",
    "    if data is not None:\n",
    "        # Update the dataframe with data from the API\n",
    "        if \"author\" in data[\"message\"]:\n",
    "            message = data[\"message\"][\"author\"]\n",
    "            for i in range(len(message)):\n",
    "                if \"name\" in message[i]['affiliation']:\n",
    "                    test_author.loc[test_author['last_name'] == message[i]['family'], 'affiliation'] = message[i]['affiliation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b2968-76e2-4098-b7f4-0f1c4fb7e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634ca15-df42-4281-85d2-d6425d5e0bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66dba2b-b946-4df0-9400-d13931f11861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory 'tables'\n",
    "!mkdir tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c7b42-4490-4580-baac-0d508201cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorship.to_csv('tables/authorship.csv', index = False)\n",
    "article_category.to_csv('tables/article_category.csv', index = False)\n",
    "category.to_csv('tables/category.csv', index = False)\n",
    "journal.to_csv('tables/journal.csv', index = False)\n",
    "article.to_csv('tables/article.csv', index = False)\n",
    "author.to_csv('tables/author.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb1a21",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f16e9b-59ad-4228-9f63-262caf4c1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "authorship = pd.read_csv('tables/authorship.csv')\n",
    "article_category = pd.read_csv('tables/article_category.csv')\n",
    "category = pd.read_csv('tables/category.csv')\n",
    "article = pd.read_csv('tables/article.csv')\n",
    "author = pd.read_csv('tables/author.csv')\n",
    "journal = pd.read_csv('tables/journal.csv')\n",
    "\n",
    "tables = [authorship, article_category, category, article, author, journal]\n",
    "\n",
    "# Name of tables (for later print)\n",
    "authorship.name = 'authorship'\n",
    "article_category.name = 'article_category'\n",
    "category.name = 'category'\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "journal.name = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89207db2-6b21-41f8-9e23-437659715296",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f21ed2-6242-4fc1-a470-738e700a767e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e947cbe-2492-4ab3-8913-d93fe37dd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42b82d-21cf-4616-a782-aacd3a96fb49",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fcc14-51f5-4bf0-822e-5e37213b672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af407c-d558-4db7-8462-0f517a4b0930",
   "metadata": {},
   "source": [
    "# Drop Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42449003-4ae7-40dd-ac46-a80009ef0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863726dc-c871-4c4e-a0f5-21b2fc099b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that a table, e.g., 'jounal', is not in the database\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d295154-cef9-4072-a8e0-c0dbe1c49397",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb500397-7d81-45c3-9107-eeee41f10836",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_tables:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd37c3c-87b8-41ff-9119-bb2b3f33c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the tables (e.g., 'author') are created\n",
    "## Should be empty\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca09335-0049-41da-96a4-bdb0fb7a9c3d",
   "metadata": {},
   "source": [
    "# Insert into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c3e6e-2ba3-41cf-ae0f-3a7ab42d96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()\n",
    "        \n",
    "for  i in range(len(tables)):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6ee8b-ccbe-4ce4-8bfe-98b0cfc5c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM author LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6235b-b285-474c-9596-d006feca6061",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530a291-0ebd-4af0-a999-bc1337caf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a5d25-1d54-42df-b237-9eec0fad2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653ec78-1f13-406e-b64a-08482bac57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbef25-7b1b-4030-bc61-bab4ac2dec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d039967-1cbb-479e-a3e1-515ac318be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78fe71",
   "metadata": {},
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097504f",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c02c51-46c8-4b8a-9098-234bb2ca0cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18104987",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce61fb1",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255073e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d1c7378",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760e100-abb5-4495-aec3-322019757912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install \"dask[complete]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba87465-8d19-48a2-8026-0885d2968bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dab965-6cd8-4dd0-89bd-3a51a210818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# use it with chunks\n",
    "#df = dd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345e455-3f0e-49ef-b2a0-42b270ad2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw  = pd.read_json(\"arxiv/arxiv-metadata-oai-snapshot.json\", lines = True, nrows = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad934756-86cd-4919-a11b-6fc016c23d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23469c0b-2561-49bb-90bb-25419098fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[~df_raw['doi'].isnull()]\n",
    "df_raw = df_raw[(df_raw['categories'].str.contains('cs.')) & (~df_raw['categories'].str.contains('physics'))].reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe3288-965b-4543-b559-ab23986997d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935f018-cb71-441b-85eb-612acf58afe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72c12a-efc7-42bb-b072-8b3322a844e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739d065-46b3-42f7-96a7-9a8345a71a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f0bce8-0dcd-4554-b748-5c8531d07c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6ab0c-bb3b-4239-8b6d-beefe3a891b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
