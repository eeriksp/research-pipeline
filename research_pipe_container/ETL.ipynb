{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c66539",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81cb94",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abd105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install psycopg2 -y\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ec39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operationsõ\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "from math import floor\n",
    "import time\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "from unidecode import unidecode # international encoding fo names\n",
    "\n",
    "### Custom Scripts (ETL, augmentations, SQL)\n",
    "from scripts.raw_to_tables import *\n",
    "from scripts.augmentations import *\n",
    "from scripts.sql_queries import *\n",
    "\n",
    "### Database drivers\n",
    "import psycopg2\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8290195",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de529e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables exist...\n",
      "Tables are in the working directory!\n"
     ]
    }
   ],
   "source": [
    "# First check if the tables are already in the system\n",
    "## If tables exist, import from .csv\n",
    "\n",
    "if os.path.exists('./tables') and len(os.listdir('./tables')) == 8: # directory + 7 tables\n",
    "    print('Tables exist...')\n",
    "    author = pd.read_csv('./tables/author.csv')\n",
    "    authorshiphip = pd.read_csv('./tables/authorship.csv')\n",
    "    article = pd.read_csv('./tables/article.csv')\n",
    "    article_category = pd.read_csv('./tables/article_category.csv')\n",
    "    category = pd.read_csv('./tables/category.csv')\n",
    "    journal = pd.read_csv('./tables/journal.csv')\n",
    "    print('Tables are in the working directory!')\n",
    "    \n",
    "## If tables do not exist, pull from kaggle (or local machine), proprocess to tables\n",
    "else: \n",
    "    print('Preparing tables...')\n",
    "    print()\n",
    "    ingest_and_prepare()\n",
    "    print('Tables are in the working directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c856fad",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f69a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5d3bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Article\n",
    "In this section, we use the `requests` library to fetch the citation based onthe Crossref URL of the work's DOI. We have found that this method is faster than querying the Crossref API. We extract the work type and the number of citations that the work has received; additionally, the journal ISSN for the publication is retrieved if it is available.\n",
    "\n",
    "We want to note that although we initially also wanted to fetch author affiliation, it is not really feasible, as most of this information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa03e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB! Must follow the sequence below\n",
    "\n",
    "# Check if clean 'article' table exists\n",
    "if os.path.exists('./data_ready/article.csv'):\n",
    "    article = pd.read_csv('./data_ready/article.csv')\n",
    "else:\n",
    "    # Check if augments already done\n",
    "    if os.path.exists('./tables/article_augmented_raw.csv'):\n",
    "        article = pd.read_csv('./tables/article_augmented_raw.csv')\n",
    "    else:\n",
    "        # Candidate for citation updating!!\n",
    "        article = pd.read_csv('./tables/article.csv')\n",
    "\n",
    "        # Use Crossref API for extracting cites, paper type, and journal ISSNs\n",
    "        batches = range(0, len(article), 2000)\n",
    "        for b in batches:\n",
    "            start_range = b\n",
    "            end_range = b + 2000\n",
    "            # Use the custom augmentation script\n",
    "            ## NB! 5k records in appx 30min, 2k records in appx 14min \n",
    "            fetch_article_augments(start_range, end_range)\n",
    "        # Last batch\n",
    "        print(time.ctime())\n",
    "        start_article = time.time()\n",
    "        start_range = batches[-1]\n",
    "        end_range = len(article)\n",
    "        fetch_article_augments(start_range, end_range)\n",
    "        end_article = time.time()\n",
    "        end_article - start_article/60\n",
    "        end_batches = time.time()\n",
    "        print(f'End of article augmentation: {end_batches}')\n",
    "\n",
    "        # Write to a separate csv (without filtering\n",
    "        article.to_csv('tables/article_augmented_raw.csv', index = False)\n",
    "    \n",
    "    # Include only journal articles\n",
    "    article_journal = article[article['type'] == 'journal-article'].reset_index(drop = True)\n",
    "    \n",
    "    # Write to 'data_ready' directory\n",
    "    article_journal.to_csv('./data_ready/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ebf763",
   "metadata": {},
   "source": [
    "## Journal (and article update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a6cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/journal.csv'):\n",
    "    journal = pd.read_csv('./data_ready/journal.csv')\n",
    "else:\n",
    "    journal = pd.read_csv('tables/journal.csv')\n",
    "    \n",
    "    # Import the journal database data\n",
    "    ## NB! It may take some time\n",
    "    print('Importing CWTS data (2021)...')\n",
    "    cwts_data = pd.read_excel('augmentation/CWTS Journal Indicators April 2022.xlsx',\n",
    "                         sheet_name = 'Sources')\n",
    "    # Fix colnames (replace spaces and lower)\n",
    "    cwts_data.columns = [col.replace(' ','_').lower() for col in cwts_data.columns] \n",
    "    # Include only 2021 records\n",
    "    cwts21 = cwts_data[cwts_data['year'].isin([2021])].reset_index(drop = True)\n",
    "    print('CWTS (2021) data imported!')\n",
    "    \n",
    "    # Find the journals\n",
    "    journal['journal_issn'] = article['journal_issn'].unique() # NB!! 'article' must be in pwd\n",
    "    journal = journal[~journal['journal_issn'].isnull()] # remove NAs\n",
    "    journal = journal.sort_values('journal_issn').reset_index(drop = True)\n",
    "    \n",
    "    print(f'The number of unique journals: {len(journal)}')\n",
    "    journal = find_journal_stats(journal, cwts21) # from augmentations.py\n",
    "    \n",
    "    print('Writing a clean journal.csv')\n",
    "    journal.to_csv('./data_ready/journal.csv', index = False)\n",
    "    print(\"'journal.csv' written to 'data_ready' directory!\")\n",
    "\n",
    "# Remove not found journals from articles\n",
    "article = article[article['journal_issn'].isin(journal['journal_issn'])].reset_index(drop = True)\n",
    "# Update 'article.csv' in 'data_ready' directory\n",
    "article.to_csv('./data_ready/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b73b8",
   "metadata": {},
   "source": [
    "### Authorship update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f831c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/authorship.csv'):\n",
    "    authorship = pd.read_csv('./data_ready/authorship.csv')\n",
    "else:\n",
    "    authorship = pd.read_csv('./tables/authorship.csv')\n",
    "    # Include only the relations in 'article' table\n",
    "    authorship = authorship[authorship['article_id'].isin(article['article_id'])].sort_values('article_id').reset_index(drop = True)\n",
    "    # Write to csv\n",
    "    authorship.to_csv('./data_ready/authorship.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ae753",
   "metadata": {},
   "source": [
    "### Author update and augments\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f167ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdb438e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/author.csv'):\n",
    "    author = pd.read_csv('./data_ready/author.csv')\n",
    "else:\n",
    "    author = pd.read_csv('./tables/author.csv').drop_duplicates()\n",
    "    \n",
    "    # Filter authors\n",
    "    author = author[author['author_id'].isin(authorship['author_id'])].drop_duplicates(['author_id']).reset_index(drop = True)\n",
    "    \n",
    "    # Augment gender\n",
    "    print('Importing gender information...')\n",
    "    names_genders = pd.read_csv('./augmentation/names_genders.csv')[['first_name', 'gender']]\n",
    "    author = author.merge(names_genders, on = 'first_name', how = 'left')\n",
    "    print('Gender augmentation done where possible')\n",
    "    \n",
    "    # Number of publications\n",
    "    npubs = pd.DataFrame(authorship.reset_index(drop = True).groupby('author_id').size()).sort_values('author_id').reset_index()\n",
    "    npubs.columns = ['author_id', 'total_pubs']\n",
    "    author = author.merge(npubs, on = 'author_id')\n",
    "    \n",
    "    # Additional augments\n",
    "    ## Statistics table\n",
    "    stats = authorship.merge(article[['article_id', 'n_cites', 'n_authors']], on = 'article_id').sort_values('author_id').reset_index(drop = True)\n",
    "\n",
    "    ## Add new columns to author table\n",
    "    author['total_cites'] = np.zeros(len(author))\n",
    "    author['avg_cites'] = np.zeros(len(author))\n",
    "    author['med_coauthors'] = np.zeros(len(author))\n",
    "    author['n_unique_coauthors'] = np.zeros(len(author))\n",
    "    author['hindex'] = np.zeros(len(author))\n",
    "    \n",
    "    ## Add statistics to authors\n",
    "    ### NB! Slow run...\n",
    "    print('Computing author statistics...')\n",
    "    for i in tqdm(range(len(author))):    \n",
    "        author_id = author.loc[i, 'author_id']\n",
    "        papers = stats[stats['author_id'] == author_id].sort_values('n_cites').reset_index(drop = True)\n",
    "        citations = papers['n_cites'].sort_values(ascending = False).reset_index(drop = True)\n",
    "\n",
    "        # The nyumber of unique co-authors\n",
    "        articles = authorship[authorship['author_id'] == author_id]['article_id']\n",
    "        author.loc[i, 'n_unique_coauthors'] = authorship[authorship['article_id'].isin(articles)]['author_id'].unique().size - 1\n",
    "\n",
    "            # Stats\n",
    "        author.loc[i, 'total_cites'] = papers['n_cites'].sum() # Total number of citations\n",
    "        author.loc[i, 'avg_cites'] = round(author.loc[i, 'total_cites']/len(papers),3) # Average number of citations per paper\n",
    "        author.loc[i, 'med_coauthors'] = np.median(papers['n_authors']-1) # subtract oneself\n",
    "\n",
    "            # h-index\n",
    "        author.loc[i, 'hindex'] = hindex(citations, len(citations))\n",
    "    \n",
    "    # Compute stats-based ranks\n",
    "    author['rank_total_pubs'] = author['total_pubs'].rank(ascending = 0).values.astype(int)\n",
    "    author['rank_total_cites'] = author['total_cites'].rank(ascending = 0).values.astype(int)\n",
    "    author['rank_avg_cites'] = author['avg_cites'].rank(ascending = 0).values.astype(int)\n",
    "    author['rank_hindex'] = author['hindex'].rank(ascending = 0).values.astype(int)\n",
    "    author = author.sort_values('rank_hindex').reset_index(drop = True) # sort by h-index\n",
    "    \n",
    "    # Format data types\n",
    "    author['total_cites'] = author['total_cites'].astype(int)\n",
    "    author['n_unique_coauthors'] = author['n_unique_coauthors'].astype(int)\n",
    "    author['hindex'] = author['hindex'].astype(int)\n",
    "    \n",
    "    print('Computing done!') \n",
    "    print('Saving author table to .csv...') \n",
    "    # Save to csv\n",
    "    author.to_csv('./data_ready/author.csv', index = False)\n",
    "    print('author table saved!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518f4af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>total_pubs</th>\n",
       "      <th>total_cites</th>\n",
       "      <th>avg_cites</th>\n",
       "      <th>med_coauthors</th>\n",
       "      <th>n_unique_coauthors</th>\n",
       "      <th>hindex</th>\n",
       "      <th>rank_total_pubs</th>\n",
       "      <th>rank_total_cites</th>\n",
       "      <th>rank_avg_cites</th>\n",
       "      <th>rank_hindex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WangX</td>\n",
       "      <td>Wang</td>\n",
       "      <td>Xingbo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167</td>\n",
       "      <td>6958</td>\n",
       "      <td>41.665</td>\n",
       "      <td>3.0</td>\n",
       "      <td>349</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PoorH</td>\n",
       "      <td>Poor</td>\n",
       "      <td>H</td>\n",
       "      <td>Vincent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>6876</td>\n",
       "      <td>88.154</td>\n",
       "      <td>3.0</td>\n",
       "      <td>147</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>2802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZhangJ</td>\n",
       "      <td>Zhang</td>\n",
       "      <td>Jiawei</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>156</td>\n",
       "      <td>6449</td>\n",
       "      <td>41.340</td>\n",
       "      <td>3.0</td>\n",
       "      <td>350</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>6651</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AbramoG</td>\n",
       "      <td>Abramo</td>\n",
       "      <td>Giovanni</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>96</td>\n",
       "      <td>2941</td>\n",
       "      <td>30.635</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>90</td>\n",
       "      <td>9238</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LiuY</td>\n",
       "      <td>Liu</td>\n",
       "      <td>Ying</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "      <td>5562</td>\n",
       "      <td>34.981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>398</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>8057</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56197</th>\n",
       "      <td>MurrayH</td>\n",
       "      <td>Murray</td>\n",
       "      <td>Hazel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37927</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56198</th>\n",
       "      <td>MurrayY</td>\n",
       "      <td>Murray</td>\n",
       "      <td>Yvonne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37927</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56199</th>\n",
       "      <td>MurtaA</td>\n",
       "      <td>Murta</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37927</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56200</th>\n",
       "      <td>MunzL</td>\n",
       "      <td>Munz</td>\n",
       "      <td>Leon</td>\n",
       "      <td>Paul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37927</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56201</th>\n",
       "      <td>LehnerS</td>\n",
       "      <td>Lehner</td>\n",
       "      <td>Sebastian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>37927</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "      <td>52903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56202 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      author_id last_name first_name middle_name gender  total_pubs  \\\n",
       "0         WangX      Wang     Xingbo         NaN    NaN         167   \n",
       "1         PoorH      Poor          H     Vincent    NaN          78   \n",
       "2        ZhangJ     Zhang     Jiawei         NaN      F         156   \n",
       "3       AbramoG    Abramo   Giovanni         NaN      M          96   \n",
       "4          LiuY       Liu       Ying         NaN    NaN         159   \n",
       "...         ...       ...        ...         ...    ...         ...   \n",
       "56197   MurrayH    Murray      Hazel         NaN      F           1   \n",
       "56198   MurrayY    Murray     Yvonne         NaN      F           1   \n",
       "56199    MurtaA     Murta     Arthur           H      M           1   \n",
       "56200     MunzL      Munz       Leon        Paul    NaN           1   \n",
       "56201   LehnerS    Lehner  Sebastian         NaN      F           1   \n",
       "\n",
       "       total_cites  avg_cites  med_coauthors  n_unique_coauthors  hindex  \\\n",
       "0             6958     41.665            3.0                 349      39   \n",
       "1             6876     88.154            3.0                 147      39   \n",
       "2             6449     41.340            3.0                 350      37   \n",
       "3             2941     30.635            2.0                  20      33   \n",
       "4             5562     34.981            3.0                 398      33   \n",
       "...            ...        ...            ...                 ...     ...   \n",
       "56197            0      0.000            1.0                   1       0   \n",
       "56198            0      0.000            4.0                   4       0   \n",
       "56199            0      0.000            1.0                   1       0   \n",
       "56200            0      0.000            1.0                   1       0   \n",
       "56201            0      0.000            5.0                   5       0   \n",
       "\n",
       "       rank_total_pubs  rank_total_cites  rank_avg_cites  rank_hindex  \n",
       "0                    2                10            6632            1  \n",
       "1                   20                11            2802            1  \n",
       "2                    4                14            6651            3  \n",
       "3                    9                90            9238            5  \n",
       "4                    3                27            8057            5  \n",
       "...                ...               ...             ...          ...  \n",
       "56197            37927             52903           52903        52903  \n",
       "56198            37927             52903           52903        52903  \n",
       "56199            37927             52903           52903        52903  \n",
       "56200            37927             52903           52903        52903  \n",
       "56201            37927             52903           52903        52903  \n",
       "\n",
       "[56202 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a730399",
   "metadata": {},
   "source": [
    "### Article category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203e7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/article_category.csv'):\n",
    "    article_category = pd.read_csv('./data_ready/article_category.csv')\n",
    "else:\n",
    "    article_category = pd.read_csv('./tables/article_category.csv')\n",
    "    article_category = article_category[article_category['article_id'].isin(article['article_id'])].reset_index(drop = True)\n",
    "    article_category.to_csv('./data_ready/article_category.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b551ff7",
   "metadata": {},
   "source": [
    "### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717bda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data_ready/category.csv'):\n",
    "    category = pd.read_csv('./data_ready/category.csv')\n",
    "else:\n",
    "    category = pd.read_csv('./tables/category.csv')\n",
    "    category = category[category['category_id'].isin(article_category['category_id'])].reset_index(drop = True)\n",
    "    category.to_csv('./data_ready/category.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9264d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f61f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7f03ba",
   "metadata": {},
   "source": [
    "### Journal\n",
    "In order to get the journal information, we need the journal ISSN list from the `article` table. Although journal Impact Factor are more common metrics, they are trademarked and, hence, retrieving them is not open-source. The alternative is to use SNIP - source-normalized impact per publication. This is the average number of citations per publication, corrected for differences in citation practice between research domains. Fortunately, the list of journals and their SNIP is available from the CWTS website (https://www.journalindicators.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5396671",
   "metadata": {},
   "source": [
    "#### Merge author-names-genders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86dccb",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c34bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "article = pd.read_csv('data_ready/article.csv')\n",
    "author = pd.read_csv('data_ready/author.csv')\n",
    "authorship = pd.read_csv('data_ready/authorship.csv')\n",
    "category = pd.read_csv('data_ready/category.csv')\n",
    "article_category = pd.read_csv('data_ready/article_category.csv')\n",
    "journal = pd.read_csv('data_ready/journal.csv')\n",
    "\n",
    "tables = [article, author, authorship, category, article_category, journal]\n",
    "\n",
    "# Name of tables (for later print)\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "authorship.name = 'authorship'\n",
    "category.name = 'category'\n",
    "article_category.name = 'article_category'\n",
    "journal.name = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c59e8e-a331-477e-9b64-36c67172df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into tables (helper function)\n",
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb64cb95-b5d1-4a6a-999b-5b201e1657a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "    # create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5470dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100a97c9-4b92-4637-81bf-326072bce043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()\n",
    "    \n",
    "    # Create Tables\n",
    "for query in create_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b23e79-8a93-4cf9-8caf-f725e2a0b501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting table -- article -- ...\n",
      "Table -- article -- successfully inserted!\n",
      "\n",
      "Inserting table -- author -- ...\n",
      "Table -- author -- successfully inserted!\n",
      "\n",
      "Inserting table -- authorship -- ...\n",
      "Table -- authorship -- successfully inserted!\n",
      "\n",
      "Inserting table -- category -- ...\n",
      "Table -- category -- successfully inserted!\n",
      "\n",
      "Inserting table -- article_category -- ...\n",
      "Table -- article_category -- successfully inserted!\n",
      "\n",
      "Inserting table -- journal -- ...\n",
      "Table -- journal -- successfully inserted!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert into tables\n",
    "for i in tqdm(range(len(tables))):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35803705",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d08189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install psycogp2\n",
    "# !pip install ipython-sql\n",
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58660370",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae10a0",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abef611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://postgres:***@postgres/postgres\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>article_id</th>\n",
       "        <th>title</th>\n",
       "        <th>doi</th>\n",
       "        <th>n_authors</th>\n",
       "        <th>journal_issn</th>\n",
       "        <th>type</th>\n",
       "        <th>n_cites</th>\n",
       "        <th>year</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad266429",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e327694-a1a3-4129-bb26-83a85b5e74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph,Node,Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fbf851b-8097-4568-8710-38f405853ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Connection to Neo4j DB!!\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "try:\n",
    "    graph = Graph(\"bolt://neo:7687\")\n",
    "except:\n",
    "    print(\"Error Connection to Neo4j DB!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c415d",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ba49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "261caa03",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aed471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dbc48ec",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae873bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0313bf5",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea58d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
